{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMJuHdISuRN3tzZo0Ewk4w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DemoySegment/dl-miniproject-resnet/blob/main/dl_miniproject_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfi-diP8Ivpj",
        "outputId": "86dcfc4e-09c4-44d8-9dd8-912cd52d8fc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.2\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchsummary\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as functional\n",
        "import torchvision.models as models\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import ConcatDataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# output_size = (input_size + 2*padding - kernel)/stride + 1 \n",
        "class BuildingBlock(nn.Module):\n",
        "   \n",
        "    def __init__(self, in_channels, intermediate_channels, identity_downsample=None, kernel_size=3, stride=1, expansion=1):\n",
        "      \"\"\"\n",
        "      This class is for building a resnet block. In each block various \n",
        "      convolution layers will be connected to each other with a batctnorm layer and a relu activation between.\n",
        "      Skip connection will be built between the input of the first layer and the input of the last layer, \n",
        "      that is to add the input of the block to the output of the last batctnorm layer.\n",
        "      Size of the two inputs of skip connection should be pay attention to.\n",
        "\n",
        "      :param in_channels: the number of input channels of the whole block. Since block will repeat several times, \n",
        "      lets say a block with with a input channels of 64 and output channels of 128, the next time going\n",
        "      through the block need a input channels of 128.\n",
        "\n",
        "      :param intermediate_channels: the number of output channels of conv layers in the block. \n",
        "      Since channels always expand, the output channels of the block will be the expansion * intermediate_channels.\n",
        "\n",
        "      :param identity_downsample: a model to deal with skip connection problem. this model should have a conv layer and\n",
        "      a batchnorm layer. In the next iteration of same block, the input channels may not be consist with the output of the \n",
        "      last batchnorm output, therefore we need the parameter to help change x's channels.\n",
        "      :type identity_downsample: nn.Module\n",
        "\n",
        "      :paran stride: if stride>1 for one conv layer in each same block in iteration, then the size of the images will be \n",
        "      decreased for block_num times, which is not what we want. Therefore, for iterations of the the same block, only one layer\n",
        "      in one of the block will have a stride that reduce the size of the image.\n",
        "      \"\"\"\n",
        "\n",
        "      super(BuildingBlock, self).__init__()\n",
        "      \n",
        "      #expansion rate, the output channels of the block will be the expansion * intermediate_channels\n",
        "      self.expansion = expansion\n",
        "      self.conv1 = nn.Conv2d(\n",
        "          in_channels,\n",
        "          intermediate_channels,\n",
        "          kernel_size=kernel_size,\n",
        "          stride=stride,\n",
        "          padding=(int)((kernel_size-1)/2),\n",
        "          bias=False,\n",
        "      )\n",
        "      self.bn1 = nn.BatchNorm2d(intermediate_channels)\n",
        "      self.conv2 = nn.Conv2d(\n",
        "          intermediate_channels,\n",
        "          intermediate_channels * self.expansion,\n",
        "          kernel_size=kernel_size,\n",
        "          stride=1,\n",
        "          padding=(int)((kernel_size-1)/2),\n",
        "          bias=False,\n",
        "      )\n",
        "      self.bn2 = nn.BatchNorm2d(intermediate_channels * self.expansion)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.identity_downsample = identity_downsample\n",
        "      self.stride = stride\n",
        "      self.in_channels = in_channels\n",
        "      self.intermediate_channels = intermediate_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(identity)\n",
        "\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layerNums, image_channels, start_channels, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "        # head layers\n",
        "        self.in_channels = start_channels\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            image_channels, self.in_channels, kernel_size=3, stride=1, padding=1, bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # recursion block layers\n",
        "        # Essentially the entire ResNet architecture are in these 4 lines below\n",
        "        self.layer1, self.in_channels = self._make_block(\n",
        "            BuildingBlock, layerNums[0], intermediate_channels=32, in_channels=self.in_channels, stride=1, kernel_size=5\n",
        "        )\n",
        "        self.layer2, self.in_channels = self._make_block(\n",
        "            BuildingBlock, layerNums[1], intermediate_channels=64, in_channels=self.in_channels, stride=2, kernel_size=5\n",
        "        )\n",
        "        self.layer3, self.in_channels = self._make_block(\n",
        "            BuildingBlock, layerNums[2], intermediate_channels=128, in_channels=self.in_channels, stride=2, kernel_size=3\n",
        "        )\n",
        "        self.layer4, self.in_channels = self._make_block(\n",
        "            BuildingBlock, layerNums[3], intermediate_channels=256, in_channels=self.in_channels, stride=2, kernel_size=3\n",
        "        )\n",
        "\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "        self.layerNums = layerNums\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        #x = self.maxpool(x)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        #x = functional.softmax(x, dim=0)\n",
        "        return x\n",
        "\n",
        "    # for resnet18, expansion === 1\n",
        "    def _make_block(self, block, num_layers, intermediate_channels, in_channels, stride, expansion=1, kernel_size=3):\n",
        "        identity_downsample = None\n",
        "        layers = []\n",
        "\n",
        "        # Either if we half the input space for ex, 56x56 -> 28x28 (stride=2), or channels changes\n",
        "        # we need to adapt the Identity (skip connection) so it will be able to be added\n",
        "        # to the layer that's ahead\n",
        "        # it is used at the end of first iteration of each block\n",
        "        if stride != 1 or in_channels != intermediate_channels*expansion:\n",
        "          identity_downsample = nn.Sequential(\n",
        "                  nn.Conv2d(\n",
        "                      in_channels,\n",
        "                      intermediate_channels*expansion,\n",
        "                      kernel_size=1,\n",
        "                      stride=stride,\n",
        "                      bias=False,\n",
        "                  ),\n",
        "                  nn.BatchNorm2d(intermediate_channels*expansion),\n",
        "              )\n",
        "\n",
        "        layers.append(\n",
        "            block(in_channels, intermediate_channels, stride=stride, identity_downsample=identity_downsample, expansion=expansion, kernel_size=kernel_size)\n",
        "        )\n",
        "\n",
        "       \n",
        "        in_channels = intermediate_channels*expansion\n",
        "\n",
        "        # For example for first resnet layer: 256 will be mapped to 64 as intermediate layer,\n",
        "        # then finally back to 256. Hence no identity downsample is needed, since stride = 1,\n",
        "        # and also same amount of channels.\n",
        "        for i in range(num_layers - 1):\n",
        "            layers.append(block(in_channels, intermediate_channels, expansion=expansion, kernel_size=kernel_size))\n",
        "        \n",
        "        return nn.Sequential(*layers), in_channels\n",
        "    \n",
        "    def to_string(self):\n",
        "      print('current model status:')\n",
        "      print('parameter numbers: {}'.format(sum(p.numel() for p in self.parameters() if p.requires_grad)))\n",
        "      print('block numbers: {}'.format(self.layerNums))"
      ],
      "metadata": {
        "id": "93XJLlMhJrD_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet(BuildingBlock, [2,4,3,3], 3, 32, 10)\n",
        "model.load_state_dict(torch.load('model.pt'))\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Be3K6zN5JUC4",
        "outputId": "0851ed7d-c033-403b-9a9c-2f35d5a038b7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU()\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BuildingBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (1): BuildingBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BuildingBlock(\n",
            "      (conv1): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "      (identity_downsample): Sequential(\n",
            "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BuildingBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (2): BuildingBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (3): BuildingBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BuildingBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "      (identity_downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BuildingBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (2): BuildingBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BuildingBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "      (identity_downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BuildingBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (2): BuildingBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, input_size=(256, 3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNHISHi1KNCk",
        "outputId": "3332995d-baad-496d-ee5d-3b5f895e381a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
            "/usr/local/lib/python3.9/dist-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return super().__sizeof__() + self.nbytes()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "ResNet                                   [256, 10]                 --\n",
              "├─Conv2d: 1-1                            [256, 32, 32, 32]         864\n",
              "├─BatchNorm2d: 1-2                       [256, 32, 32, 32]         64\n",
              "├─ReLU: 1-3                              [256, 32, 32, 32]         --\n",
              "├─Sequential: 1-4                        [256, 32, 32, 32]         --\n",
              "│    └─BuildingBlock: 2-1                [256, 32, 32, 32]         --\n",
              "│    │    └─Conv2d: 3-1                  [256, 32, 32, 32]         25,600\n",
              "│    │    └─BatchNorm2d: 3-2             [256, 32, 32, 32]         64\n",
              "│    │    └─ReLU: 3-3                    [256, 32, 32, 32]         --\n",
              "│    │    └─Conv2d: 3-4                  [256, 32, 32, 32]         25,600\n",
              "│    │    └─BatchNorm2d: 3-5             [256, 32, 32, 32]         64\n",
              "│    │    └─ReLU: 3-6                    [256, 32, 32, 32]         --\n",
              "│    └─BuildingBlock: 2-2                [256, 32, 32, 32]         --\n",
              "│    │    └─Conv2d: 3-7                  [256, 32, 32, 32]         25,600\n",
              "│    │    └─BatchNorm2d: 3-8             [256, 32, 32, 32]         64\n",
              "│    │    └─ReLU: 3-9                    [256, 32, 32, 32]         --\n",
              "│    │    └─Conv2d: 3-10                 [256, 32, 32, 32]         25,600\n",
              "│    │    └─BatchNorm2d: 3-11            [256, 32, 32, 32]         64\n",
              "│    │    └─ReLU: 3-12                   [256, 32, 32, 32]         --\n",
              "├─Sequential: 1-5                        [256, 64, 16, 16]         --\n",
              "│    └─BuildingBlock: 2-3                [256, 64, 16, 16]         --\n",
              "│    │    └─Conv2d: 3-13                 [256, 64, 16, 16]         51,200\n",
              "│    │    └─BatchNorm2d: 3-14            [256, 64, 16, 16]         128\n",
              "│    │    └─ReLU: 3-15                   [256, 64, 16, 16]         --\n",
              "│    │    └─Conv2d: 3-16                 [256, 64, 16, 16]         102,400\n",
              "│    │    └─BatchNorm2d: 3-17            [256, 64, 16, 16]         128\n",
              "│    │    └─Sequential: 3-18             [256, 64, 16, 16]         2,176\n",
              "│    │    └─ReLU: 3-19                   [256, 64, 16, 16]         --\n",
              "│    └─BuildingBlock: 2-4                [256, 64, 16, 16]         --\n",
              "│    │    └─Conv2d: 3-20                 [256, 64, 16, 16]         102,400\n",
              "│    │    └─BatchNorm2d: 3-21            [256, 64, 16, 16]         128\n",
              "│    │    └─ReLU: 3-22                   [256, 64, 16, 16]         --\n",
              "│    │    └─Conv2d: 3-23                 [256, 64, 16, 16]         102,400\n",
              "│    │    └─BatchNorm2d: 3-24            [256, 64, 16, 16]         128\n",
              "│    │    └─ReLU: 3-25                   [256, 64, 16, 16]         --\n",
              "│    └─BuildingBlock: 2-5                [256, 64, 16, 16]         --\n",
              "│    │    └─Conv2d: 3-26                 [256, 64, 16, 16]         102,400\n",
              "│    │    └─BatchNorm2d: 3-27            [256, 64, 16, 16]         128\n",
              "│    │    └─ReLU: 3-28                   [256, 64, 16, 16]         --\n",
              "│    │    └─Conv2d: 3-29                 [256, 64, 16, 16]         102,400\n",
              "│    │    └─BatchNorm2d: 3-30            [256, 64, 16, 16]         128\n",
              "│    │    └─ReLU: 3-31                   [256, 64, 16, 16]         --\n",
              "│    └─BuildingBlock: 2-6                [256, 64, 16, 16]         --\n",
              "│    │    └─Conv2d: 3-32                 [256, 64, 16, 16]         102,400\n",
              "│    │    └─BatchNorm2d: 3-33            [256, 64, 16, 16]         128\n",
              "│    │    └─ReLU: 3-34                   [256, 64, 16, 16]         --\n",
              "│    │    └─Conv2d: 3-35                 [256, 64, 16, 16]         102,400\n",
              "│    │    └─BatchNorm2d: 3-36            [256, 64, 16, 16]         128\n",
              "│    │    └─ReLU: 3-37                   [256, 64, 16, 16]         --\n",
              "├─Sequential: 1-6                        [256, 128, 8, 8]          --\n",
              "│    └─BuildingBlock: 2-7                [256, 128, 8, 8]          --\n",
              "│    │    └─Conv2d: 3-38                 [256, 128, 8, 8]          73,728\n",
              "│    │    └─BatchNorm2d: 3-39            [256, 128, 8, 8]          256\n",
              "│    │    └─ReLU: 3-40                   [256, 128, 8, 8]          --\n",
              "│    │    └─Conv2d: 3-41                 [256, 128, 8, 8]          147,456\n",
              "│    │    └─BatchNorm2d: 3-42            [256, 128, 8, 8]          256\n",
              "│    │    └─Sequential: 3-43             [256, 128, 8, 8]          8,448\n",
              "│    │    └─ReLU: 3-44                   [256, 128, 8, 8]          --\n",
              "│    └─BuildingBlock: 2-8                [256, 128, 8, 8]          --\n",
              "│    │    └─Conv2d: 3-45                 [256, 128, 8, 8]          147,456\n",
              "│    │    └─BatchNorm2d: 3-46            [256, 128, 8, 8]          256\n",
              "│    │    └─ReLU: 3-47                   [256, 128, 8, 8]          --\n",
              "│    │    └─Conv2d: 3-48                 [256, 128, 8, 8]          147,456\n",
              "│    │    └─BatchNorm2d: 3-49            [256, 128, 8, 8]          256\n",
              "│    │    └─ReLU: 3-50                   [256, 128, 8, 8]          --\n",
              "│    └─BuildingBlock: 2-9                [256, 128, 8, 8]          --\n",
              "│    │    └─Conv2d: 3-51                 [256, 128, 8, 8]          147,456\n",
              "│    │    └─BatchNorm2d: 3-52            [256, 128, 8, 8]          256\n",
              "│    │    └─ReLU: 3-53                   [256, 128, 8, 8]          --\n",
              "│    │    └─Conv2d: 3-54                 [256, 128, 8, 8]          147,456\n",
              "│    │    └─BatchNorm2d: 3-55            [256, 128, 8, 8]          256\n",
              "│    │    └─ReLU: 3-56                   [256, 128, 8, 8]          --\n",
              "├─Sequential: 1-7                        [256, 256, 4, 4]          --\n",
              "│    └─BuildingBlock: 2-10               [256, 256, 4, 4]          --\n",
              "│    │    └─Conv2d: 3-57                 [256, 256, 4, 4]          294,912\n",
              "│    │    └─BatchNorm2d: 3-58            [256, 256, 4, 4]          512\n",
              "│    │    └─ReLU: 3-59                   [256, 256, 4, 4]          --\n",
              "│    │    └─Conv2d: 3-60                 [256, 256, 4, 4]          589,824\n",
              "│    │    └─BatchNorm2d: 3-61            [256, 256, 4, 4]          512\n",
              "│    │    └─Sequential: 3-62             [256, 256, 4, 4]          33,280\n",
              "│    │    └─ReLU: 3-63                   [256, 256, 4, 4]          --\n",
              "│    └─BuildingBlock: 2-11               [256, 256, 4, 4]          --\n",
              "│    │    └─Conv2d: 3-64                 [256, 256, 4, 4]          589,824\n",
              "│    │    └─BatchNorm2d: 3-65            [256, 256, 4, 4]          512\n",
              "│    │    └─ReLU: 3-66                   [256, 256, 4, 4]          --\n",
              "│    │    └─Conv2d: 3-67                 [256, 256, 4, 4]          589,824\n",
              "│    │    └─BatchNorm2d: 3-68            [256, 256, 4, 4]          512\n",
              "│    │    └─ReLU: 3-69                   [256, 256, 4, 4]          --\n",
              "│    └─BuildingBlock: 2-12               [256, 256, 4, 4]          --\n",
              "│    │    └─Conv2d: 3-70                 [256, 256, 4, 4]          589,824\n",
              "│    │    └─BatchNorm2d: 3-71            [256, 256, 4, 4]          512\n",
              "│    │    └─ReLU: 3-72                   [256, 256, 4, 4]          --\n",
              "│    │    └─Conv2d: 3-73                 [256, 256, 4, 4]          589,824\n",
              "│    │    └─BatchNorm2d: 3-74            [256, 256, 4, 4]          512\n",
              "│    │    └─ReLU: 3-75                   [256, 256, 4, 4]          --\n",
              "├─AdaptiveAvgPool2d: 1-8                 [256, 256, 1, 1]          --\n",
              "├─Linear: 1-9                            [256, 10]                 2,570\n",
              "==========================================================================================\n",
              "Total params: 4,978,730\n",
              "Trainable params: 4,978,730\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 104.38\n",
              "==========================================================================================\n",
              "Input size (MB): 3.15\n",
              "Forward/backward pass size (MB): 1627.41\n",
              "Params size (MB): 19.91\n",
              "Estimated Total Size (MB): 1650.47\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_pred, y):\n",
        "  predict = torch.argmax(y_pred, dim=1)\n",
        "  acc = torch.sum(predict == y) / y.shape[0]\n",
        "  return acc\n",
        "  #return y_pred.argmax(dim=1).eq(y).sum().item()"
      ],
      "metadata": {
        "id": "32H9M3A2JEOC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    #set the model in evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for(x, y) in iterator:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        y_pred = model(x)\n",
        "        loss = criterion(y_pred, y)\n",
        "        acc = accuracy(y_pred, y)\n",
        "        \n",
        "        epoch_loss += loss\n",
        "        epoch_acc += acc\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "F0EnUGLgJGHX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, scheduler):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    #set the model in training mode\n",
        "    model.train()\n",
        "\n",
        "    for(x, y) in iterator:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "\n",
        "      y_pred = model(x)\n",
        "      \n",
        "      loss = criterion(y_pred, y)\n",
        "      acc = accuracy(y_pred, y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if scheduler is not None:\n",
        "        scheduler.step()\n",
        "      \n",
        "      epoch_loss += loss\n",
        "      epoch_acc += acc\n",
        "      \n",
        "\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "PA_y4mXnJH5C"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_epoches(epoch_num, model, optimizer, criterion, trainloader, testloader, scheduler=None):\n",
        "  best_valid_acc = float(0)\n",
        "  print(\"start running\")\n",
        "  for epoch in range(N_EPOCHS):\n",
        "      print(' --Epoch {}'.format(epoch))\n",
        "      print(\" --start training--\")\n",
        "      train_loss, train_acc = train(model, trainloader, optimizer, criterion, scheduler)\n",
        "      print(\" --start validing--\")\n",
        "      valid_loss, valid_acc = evaluate(model, testloader, criterion)\n",
        "      if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "\n",
        "      print('Epoch:', epoch, 'LR:', scheduler.get_lr())\n",
        "      print(f'  \\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "      print(f'  \\t Val. Loss: {valid_loss:.3f} |  Val Acc: {valid_acc*100:.2f}%')\n",
        "      print(f'  Current best Val Acc: {best_valid_acc}')\n",
        "      torch.cuda.empty_cache()\n",
        "  print(\"--end running\")\n",
        "  return best_valid_acc"
      ],
      "metadata": {
        "id": "p5M3yVr2JJ9T"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "best_result_SGD = (0.1, 128, 0.0001)\n",
        "lr, batch_size, wd = best_result_SGD\n",
        "\n",
        "N_EPOCHS = 40\n",
        "\n",
        "print(\"--------------lr={}, batch_size={}, wd={} start-------------\".format(lr, batch_size, wd))\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=wd)\n",
        "cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=0.0001)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "transform1 = transforms.Compose([\n",
        "        \n",
        "        transforms.RandomResizedCrop(32),#训练模型有resize 和 翻折的操作\n",
        "    transforms.RandomHorizontalFlip(p=1),#\n",
        "        transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "transform2 = transforms.Compose([\n",
        "        \n",
        "        transforms.RandomResizedCrop(32),#训练模型有resize 和 翻折的操作\n",
        "    transforms.RandomVerticalFlip(p=1),\n",
        "        transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "transform3 = transforms.Compose([\n",
        "        \n",
        "        transforms.RandomResizedCrop(32),#训练模型有resize 和 翻折的操作\n",
        "    transforms.RandomRotation(90),\n",
        "        transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainset1 = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform1)\n",
        "trainset2 = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform2)\n",
        "trainset3 = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform3)\n",
        "\n",
        "trainset = ConcatDataset([trainset,trainset1,trainset3,trainset2])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0,drop_last=True)\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0,drop_last=True)\n",
        "\n",
        "_, acc = evaluate(model, testloader, criterion)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzN-4nblJPC0",
        "outputId": "bea7be6a-deb6-4bbd-a736-4e8ddc913071"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------lr=0.1, batch_size=128, wd=0.0001 start-------------\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "tensor(0.9105, device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}