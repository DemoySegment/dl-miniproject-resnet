{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DemoySegment/dl-miniproject-resnet/blob/main/dl_miniproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDPCtreRsDjn",
        "outputId": "4e4407d4-1c55-4a39-dcb4-00e756c7def2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.2\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchsummary\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as functional\n",
        "import torchvision.models as models\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import ConcatDataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8HqCmimo_HB"
      },
      "outputs": [],
      "source": [
        "# output_size = (input_size + 2*padding - kernel)/stride + 1 \n",
        "class BuildingBlock(nn.Module):\n",
        "   \n",
        "    def __init__(self, in_channels, intermediate_channels, identity_downsample=None, kernel_size=3, stride=1, expansion=1):\n",
        "      \"\"\"\n",
        "      This class is for building a resnet block. In each block various \n",
        "      convolution layers will be connected to each other with a batctnorm layer and a relu activation between.\n",
        "      Skip connection will be built between the input of the first layer and the input of the last layer, \n",
        "      that is to add the input of the block to the output of the last batctnorm layer.\n",
        "      Size of the two inputs of skip connection should be pay attention to.\n",
        "\n",
        "      :param in_channels: the number of input channels of the whole block. Since block will repeat several times, \n",
        "      lets say a block with with a input channels of 64 and output channels of 128, the next time going\n",
        "      through the block need a input channels of 128.\n",
        "\n",
        "      :param intermediate_channels: the number of output channels of conv layers in the block. \n",
        "      Since channels always expand, the output channels of the block will be the expansion * intermediate_channels.\n",
        "\n",
        "      :param identity_downsample: a model to deal with skip connection problem. this model should have a conv layer and\n",
        "      a batchnorm layer. In the next iteration of same block, the input channels may not be consist with the output of the \n",
        "      last batchnorm output, therefore we need the parameter to help change x's channels.\n",
        "      :type identity_downsample: nn.Module\n",
        "\n",
        "      :paran stride: if stride>1 for one conv layer in each same block in iteration, then the size of the images will be \n",
        "      decreased for block_num times, which is not what we want. Therefore, for iterations of the the same block, only one layer\n",
        "      in one of the block will have a stride that reduce the size of the image.\n",
        "      \"\"\"\n",
        "\n",
        "      super(BuildingBlock, self).__init__()\n",
        "      \n",
        "      #expansion rate, the output channels of the block will be the expansion * intermediate_channels\n",
        "\n",
        "      #to keep the size unchanged, padding should be set to ((kernel_size-1)/2)\n",
        "      self.expansion = expansion\n",
        "      self.conv1 = nn.Conv2d(\n",
        "          in_channels,\n",
        "          intermediate_channels,\n",
        "          kernel_size=kernel_size,\n",
        "          stride=stride,\n",
        "          padding=(int)((kernel_size-1)/2),\n",
        "          bias=False,\n",
        "      )\n",
        "      self.bn1 = nn.BatchNorm2d(intermediate_channels)\n",
        "      self.conv2 = nn.Conv2d(\n",
        "          intermediate_channels,\n",
        "          intermediate_channels * self.expansion,\n",
        "          kernel_size=kernel_size,\n",
        "          stride=1,\n",
        "          padding=(int)((kernel_size-1)/2),\n",
        "          bias=False,\n",
        "      )\n",
        "      self.bn2 = nn.BatchNorm2d(intermediate_channels * self.expansion)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.identity_downsample = identity_downsample\n",
        "      self.stride = stride\n",
        "      self.in_channels = in_channels\n",
        "      self.intermediate_channels = intermediate_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(identity)\n",
        "\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layerNums, image_channels, start_channels, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "        # head layers\n",
        "        self.in_channels = start_channels\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            image_channels, self.in_channels, kernel_size=3, stride=1, padding=1, bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # recursion block layers\n",
        "        # Essentially the entire ResNet architecture are in these 4 lines below\n",
        "        self.layer1, self.in_channels = self._make_block(\n",
        "            BuildingBlock, layerNums[0], intermediate_channels=32, in_channels=self.in_channels, stride=1, kernel_size=5\n",
        "        )\n",
        "        self.layer2, self.in_channels = self._make_block(\n",
        "            BuildingBlock, layerNums[1], intermediate_channels=64, in_channels=self.in_channels, stride=2, kernel_size=5\n",
        "        )\n",
        "        self.layer3, self.in_channels = self._make_block(\n",
        "            BuildingBlock, layerNums[2], intermediate_channels=128, in_channels=self.in_channels, stride=2, kernel_size=3\n",
        "        )\n",
        "        self.layer4, self.in_channels = self._make_block(\n",
        "            BuildingBlock, layerNums[3], intermediate_channels=256, in_channels=self.in_channels, stride=2, kernel_size=3\n",
        "        )\n",
        "\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "        self.layerNums = layerNums\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        #x = self.maxpool(x)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        #x = functional.softmax(x, dim=0)\n",
        "        return x\n",
        "\n",
        "    # for resnet18, expansion === 1\n",
        "    def _make_block(self, block, num_layers, intermediate_channels, in_channels, stride, expansion=1, kernel_size=3):\n",
        "        identity_downsample = None\n",
        "        layers = []\n",
        "\n",
        "        # Either if we half the input space for ex, 56x56 -> 28x28 (stride=2), or channels changes\n",
        "        # we need to adapt the Identity (skip connection) so it will be able to be added\n",
        "        # to the layer that's ahead\n",
        "        # it is used at the end of first iteration of each block\n",
        "        if stride != 1 or in_channels != intermediate_channels*expansion:\n",
        "          identity_downsample = nn.Sequential(\n",
        "                  nn.Conv2d(\n",
        "                      in_channels,\n",
        "                      intermediate_channels*expansion,\n",
        "                      kernel_size=1,\n",
        "                      stride=stride,\n",
        "                      bias=False,\n",
        "                  ),\n",
        "                  nn.BatchNorm2d(intermediate_channels*expansion),\n",
        "              )\n",
        "\n",
        "        layers.append(\n",
        "            block(in_channels, intermediate_channels, stride=stride, identity_downsample=identity_downsample, expansion=expansion, kernel_size=kernel_size)\n",
        "        )\n",
        "\n",
        "       \n",
        "        in_channels = intermediate_channels*expansion\n",
        "\n",
        "        # For example for first resnet layer: 256 will be mapped to 64 as intermediate layer,\n",
        "        # then finally back to 256. Hence no identity downsample is needed, since stride = 1,\n",
        "        # and also same amount of channels.\n",
        "        for i in range(num_layers - 1):\n",
        "            layers.append(block(in_channels, intermediate_channels, expansion=expansion, kernel_size=kernel_size))\n",
        "        \n",
        "        return nn.Sequential(*layers), in_channels\n",
        "    # for debug\n",
        "    def to_string(self):\n",
        "      print('current model status:')\n",
        "      print('parameter numbers: {}'.format(sum(p.numel() for p in self.parameters() if p.requires_grad)))\n",
        "      print('block numbers: {}'.format(self.layerNums))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8YnQcc6pEaG",
        "outputId": "2879abe7-8886-4708-f2c7-958d516b8f8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU()\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BuildingBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (1): BuildingBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BuildingBlock(\n",
            "      (conv1): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "      (identity_downsample): Sequential(\n",
            "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BuildingBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (2): BuildingBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (3): BuildingBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BuildingBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "      (identity_downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BuildingBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (2): BuildingBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BuildingBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "      (identity_downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BuildingBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (2): BuildingBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ResNet(BuildingBlock, [2,4,3,3], 3, 32, 10)\n",
        "model = model.to(device)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ebq9KSyk7HeK",
        "outputId": "8aa840bb-e090-410b-b5b3-e11c788b6bf7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "ResNet                                   [256, 10]                 --\n",
              "├─Conv2d: 1-1                            [256, 32, 32, 32]         864\n",
              "├─BatchNorm2d: 1-2                       [256, 32, 32, 32]         64\n",
              "├─ReLU: 1-3                              [256, 32, 32, 32]         --\n",
              "├─Sequential: 1-4                        [256, 32, 32, 32]         --\n",
              "│    └─BuildingBlock: 2-1                [256, 32, 32, 32]         --\n",
              "│    │    └─Conv2d: 3-1                  [256, 32, 32, 32]         25,600\n",
              "│    │    └─BatchNorm2d: 3-2             [256, 32, 32, 32]         64\n",
              "│    │    └─ReLU: 3-3                    [256, 32, 32, 32]         --\n",
              "│    │    └─Conv2d: 3-4                  [256, 32, 32, 32]         25,600\n",
              "│    │    └─BatchNorm2d: 3-5             [256, 32, 32, 32]         64\n",
              "│    │    └─ReLU: 3-6                    [256, 32, 32, 32]         --\n",
              "│    └─BuildingBlock: 2-2                [256, 32, 32, 32]         --\n",
              "│    │    └─Conv2d: 3-7                  [256, 32, 32, 32]         25,600\n",
              "│    │    └─BatchNorm2d: 3-8             [256, 32, 32, 32]         64\n",
              "│    │    └─ReLU: 3-9                    [256, 32, 32, 32]         --\n",
              "│    │    └─Conv2d: 3-10                 [256, 32, 32, 32]         25,600\n",
              "│    │    └─BatchNorm2d: 3-11            [256, 32, 32, 32]         64\n",
              "│    │    └─ReLU: 3-12                   [256, 32, 32, 32]         --\n",
              "├─Sequential: 1-5                        [256, 64, 16, 16]         --\n",
              "│    └─BuildingBlock: 2-3                [256, 64, 16, 16]         --\n",
              "│    │    └─Conv2d: 3-13                 [256, 64, 16, 16]         51,200\n",
              "│    │    └─BatchNorm2d: 3-14            [256, 64, 16, 16]         128\n",
              "│    │    └─ReLU: 3-15                   [256, 64, 16, 16]         --\n",
              "│    │    └─Conv2d: 3-16                 [256, 64, 16, 16]         102,400\n",
              "│    │    └─BatchNorm2d: 3-17            [256, 64, 16, 16]         128\n",
              "│    │    └─Sequential: 3-18             [256, 64, 16, 16]         2,176\n",
              "│    │    └─ReLU: 3-19                   [256, 64, 16, 16]         --\n",
              "│    └─BuildingBlock: 2-4                [256, 64, 16, 16]         --\n",
              "│    │    └─Conv2d: 3-20                 [256, 64, 16, 16]         102,400\n",
              "│    │    └─BatchNorm2d: 3-21            [256, 64, 16, 16]         128\n",
              "│    │    └─ReLU: 3-22                   [256, 64, 16, 16]         --\n",
              "│    │    └─Conv2d: 3-23                 [256, 64, 16, 16]         102,400\n",
              "│    │    └─BatchNorm2d: 3-24            [256, 64, 16, 16]         128\n",
              "│    │    └─ReLU: 3-25                   [256, 64, 16, 16]         --\n",
              "│    └─BuildingBlock: 2-5                [256, 64, 16, 16]         --\n",
              "│    │    └─Conv2d: 3-26                 [256, 64, 16, 16]         102,400\n",
              "│    │    └─BatchNorm2d: 3-27            [256, 64, 16, 16]         128\n",
              "│    │    └─ReLU: 3-28                   [256, 64, 16, 16]         --\n",
              "│    │    └─Conv2d: 3-29                 [256, 64, 16, 16]         102,400\n",
              "│    │    └─BatchNorm2d: 3-30            [256, 64, 16, 16]         128\n",
              "│    │    └─ReLU: 3-31                   [256, 64, 16, 16]         --\n",
              "│    └─BuildingBlock: 2-6                [256, 64, 16, 16]         --\n",
              "│    │    └─Conv2d: 3-32                 [256, 64, 16, 16]         102,400\n",
              "│    │    └─BatchNorm2d: 3-33            [256, 64, 16, 16]         128\n",
              "│    │    └─ReLU: 3-34                   [256, 64, 16, 16]         --\n",
              "│    │    └─Conv2d: 3-35                 [256, 64, 16, 16]         102,400\n",
              "│    │    └─BatchNorm2d: 3-36            [256, 64, 16, 16]         128\n",
              "│    │    └─ReLU: 3-37                   [256, 64, 16, 16]         --\n",
              "├─Sequential: 1-6                        [256, 128, 8, 8]          --\n",
              "│    └─BuildingBlock: 2-7                [256, 128, 8, 8]          --\n",
              "│    │    └─Conv2d: 3-38                 [256, 128, 8, 8]          73,728\n",
              "│    │    └─BatchNorm2d: 3-39            [256, 128, 8, 8]          256\n",
              "│    │    └─ReLU: 3-40                   [256, 128, 8, 8]          --\n",
              "│    │    └─Conv2d: 3-41                 [256, 128, 8, 8]          147,456\n",
              "│    │    └─BatchNorm2d: 3-42            [256, 128, 8, 8]          256\n",
              "│    │    └─Sequential: 3-43             [256, 128, 8, 8]          8,448\n",
              "│    │    └─ReLU: 3-44                   [256, 128, 8, 8]          --\n",
              "│    └─BuildingBlock: 2-8                [256, 128, 8, 8]          --\n",
              "│    │    └─Conv2d: 3-45                 [256, 128, 8, 8]          147,456\n",
              "│    │    └─BatchNorm2d: 3-46            [256, 128, 8, 8]          256\n",
              "│    │    └─ReLU: 3-47                   [256, 128, 8, 8]          --\n",
              "│    │    └─Conv2d: 3-48                 [256, 128, 8, 8]          147,456\n",
              "│    │    └─BatchNorm2d: 3-49            [256, 128, 8, 8]          256\n",
              "│    │    └─ReLU: 3-50                   [256, 128, 8, 8]          --\n",
              "│    └─BuildingBlock: 2-9                [256, 128, 8, 8]          --\n",
              "│    │    └─Conv2d: 3-51                 [256, 128, 8, 8]          147,456\n",
              "│    │    └─BatchNorm2d: 3-52            [256, 128, 8, 8]          256\n",
              "│    │    └─ReLU: 3-53                   [256, 128, 8, 8]          --\n",
              "│    │    └─Conv2d: 3-54                 [256, 128, 8, 8]          147,456\n",
              "│    │    └─BatchNorm2d: 3-55            [256, 128, 8, 8]          256\n",
              "│    │    └─ReLU: 3-56                   [256, 128, 8, 8]          --\n",
              "├─Sequential: 1-7                        [256, 256, 4, 4]          --\n",
              "│    └─BuildingBlock: 2-10               [256, 256, 4, 4]          --\n",
              "│    │    └─Conv2d: 3-57                 [256, 256, 4, 4]          294,912\n",
              "│    │    └─BatchNorm2d: 3-58            [256, 256, 4, 4]          512\n",
              "│    │    └─ReLU: 3-59                   [256, 256, 4, 4]          --\n",
              "│    │    └─Conv2d: 3-60                 [256, 256, 4, 4]          589,824\n",
              "│    │    └─BatchNorm2d: 3-61            [256, 256, 4, 4]          512\n",
              "│    │    └─Sequential: 3-62             [256, 256, 4, 4]          33,280\n",
              "│    │    └─ReLU: 3-63                   [256, 256, 4, 4]          --\n",
              "│    └─BuildingBlock: 2-11               [256, 256, 4, 4]          --\n",
              "│    │    └─Conv2d: 3-64                 [256, 256, 4, 4]          589,824\n",
              "│    │    └─BatchNorm2d: 3-65            [256, 256, 4, 4]          512\n",
              "│    │    └─ReLU: 3-66                   [256, 256, 4, 4]          --\n",
              "│    │    └─Conv2d: 3-67                 [256, 256, 4, 4]          589,824\n",
              "│    │    └─BatchNorm2d: 3-68            [256, 256, 4, 4]          512\n",
              "│    │    └─ReLU: 3-69                   [256, 256, 4, 4]          --\n",
              "│    └─BuildingBlock: 2-12               [256, 256, 4, 4]          --\n",
              "│    │    └─Conv2d: 3-70                 [256, 256, 4, 4]          589,824\n",
              "│    │    └─BatchNorm2d: 3-71            [256, 256, 4, 4]          512\n",
              "│    │    └─ReLU: 3-72                   [256, 256, 4, 4]          --\n",
              "│    │    └─Conv2d: 3-73                 [256, 256, 4, 4]          589,824\n",
              "│    │    └─BatchNorm2d: 3-74            [256, 256, 4, 4]          512\n",
              "│    │    └─ReLU: 3-75                   [256, 256, 4, 4]          --\n",
              "├─AdaptiveAvgPool2d: 1-8                 [256, 256, 1, 1]          --\n",
              "├─Linear: 1-9                            [256, 10]                 2,570\n",
              "==========================================================================================\n",
              "Total params: 4,978,730\n",
              "Trainable params: 4,978,730\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 104.38\n",
              "==========================================================================================\n",
              "Input size (MB): 3.15\n",
              "Forward/backward pass size (MB): 1627.41\n",
              "Params size (MB): 19.91\n",
              "Estimated Total Size (MB): 1650.47\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "summary(model, input_size=(256, 3, 32, 32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMYnIkQ2xGU5"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_pred, y):\n",
        "  predict = torch.argmax(y_pred, dim=1)\n",
        "  acc = torch.sum(predict == y) / y.shape[0]\n",
        "  return acc\n",
        "  #return y_pred.argmax(dim=1).eq(y).sum().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpXNbq0uxR5j"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    #set the model in evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for(x, y) in iterator:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        y_pred = model(x)\n",
        "        loss = criterion(y_pred, y)\n",
        "        acc = accuracy(y_pred, y)\n",
        "        \n",
        "        epoch_loss += loss\n",
        "        epoch_acc += acc\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8i2h32KQw5d4"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, scheduler):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    #set the model in training mode\n",
        "    model.train()\n",
        "\n",
        "    for(x, y) in iterator:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "\n",
        "      y_pred = model(x)\n",
        "      \n",
        "      loss = criterion(y_pred, y)\n",
        "      acc = accuracy(y_pred, y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if scheduler is not None:\n",
        "        scheduler.step()\n",
        "      \n",
        "      epoch_loss += loss\n",
        "      epoch_acc += acc\n",
        "      \n",
        "\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiLeiZtdgn8x"
      },
      "outputs": [],
      "source": [
        "def run_epoches(epoch_num, model, optimizer, criterion, trainloader, testloader, scheduler=None):\n",
        "  best_valid_acc = float(0)\n",
        "  print(\"start running\")\n",
        "  for epoch in range(N_EPOCHS):\n",
        "      print(' --Epoch {}'.format(epoch))\n",
        "      print(\" --start training--\")\n",
        "      train_loss, train_acc = train(model, trainloader, optimizer, criterion, scheduler)\n",
        "      print(\" --start validing--\")\n",
        "      valid_loss, valid_acc = evaluate(model, testloader, criterion)\n",
        "      if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "\n",
        "      print('Epoch:', epoch, 'LR:', scheduler.get_lr())\n",
        "      print(f'  \\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "      print(f'  \\t Val. Loss: {valid_loss:.3f} |  Val Acc: {valid_acc*100:.2f}%')\n",
        "      print(f'  Current best Val Acc: {best_valid_acc}')\n",
        "      torch.cuda.empty_cache()\n",
        "  print(\"--end running\")\n",
        "  return best_valid_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYM-HUdKxcAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50b596bf-5368-4512-a047-92a27609f105"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------lr=0.1, batch_size=128, wd=0.0001 start-------------\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "start running\n",
            " --Epoch 0\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 0 LR: [0.047616082274994406]\n",
            "  \tTrain Loss: 0.463 | Train Acc: 83.84%\n",
            "  \t Val. Loss: 0.402 |  Val Acc: 89.39%\n",
            "  Current best Val Acc: 0.8939303159713745\n",
            " --Epoch 1\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 1 LR: [0.0001988574275070351]\n",
            "  \tTrain Loss: 0.462 | Train Acc: 83.88%\n",
            "  \t Val. Loss: 0.371 |  Val Acc: 90.09%\n",
            "  Current best Val Acc: 0.9009414911270142\n",
            " --Epoch 2\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 2 LR: [0.04783552847078261]\n",
            "  \tTrain Loss: 0.461 | Train Acc: 83.86%\n",
            "  \t Val. Loss: 0.345 |  Val Acc: 90.37%\n",
            "  Current best Val Acc: 0.9037460088729858\n",
            " --Epoch 3\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 3 LR: [0.10040810228354946]\n",
            "  \tTrain Loss: 0.459 | Train Acc: 83.95%\n",
            "  \t Val. Loss: 0.405 |  Val Acc: 89.17%\n",
            "  Current best Val Acc: 0.9037460088729858\n",
            " --Epoch 4\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 4 LR: [0.060130748664876714]\n",
            "  \tTrain Loss: 0.453 | Train Acc: 84.19%\n",
            "  \t Val. Loss: 0.429 |  Val Acc: 88.71%\n",
            "  Current best Val Acc: 0.9037460088729858\n",
            " --Epoch 5\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 5 LR: [0.0020913810196123415]\n",
            "  \tTrain Loss: 0.454 | Train Acc: 84.17%\n",
            "  \t Val. Loss: 0.364 |  Val Acc: 90.12%\n",
            "  Current best Val Acc: 0.9037460088729858\n",
            " --Epoch 6\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 6 LR: [0.03558241141571472]\n",
            "  \tTrain Loss: 0.453 | Train Acc: 84.21%\n",
            "  \t Val. Loss: 0.357 |  Val Acc: 90.54%\n",
            "  Current best Val Acc: 0.9054487347602844\n",
            " --Epoch 7\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 7 LR: [0.0973076614470839]\n",
            "  \tTrain Loss: 0.450 | Train Acc: 84.32%\n",
            "  \t Val. Loss: 0.357 |  Val Acc: 89.92%\n",
            "  Current best Val Acc: 0.9054487347602844\n",
            " --Epoch 8\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 8 LR: [0.07205464323773222]\n",
            "  \tTrain Loss: 0.447 | Train Acc: 84.39%\n",
            "  \t Val. Loss: 0.431 |  Val Acc: 89.20%\n",
            "  Current best Val Acc: 0.9054487347602844\n",
            " --Epoch 9\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 9 LR: [0.006822106297601025]\n",
            "  \tTrain Loss: 0.443 | Train Acc: 84.55%\n",
            "  \t Val. Loss: 0.362 |  Val Acc: 90.07%\n",
            "  Current best Val Acc: 0.9054487347602844\n",
            " --Epoch 10\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 10 LR: [0.02432780444152411]\n",
            "  \tTrain Loss: 0.444 | Train Acc: 84.45%\n",
            "  \t Val. Loss: 0.346 |  Val Acc: 90.72%\n",
            "  Current best Val Acc: 0.907151460647583\n",
            " --Epoch 11\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 11 LR: [0.0912902842758814]\n",
            "  \tTrain Loss: 0.442 | Train Acc: 84.54%\n",
            "  \t Val. Loss: 0.410 |  Val Acc: 89.03%\n",
            "  Current best Val Acc: 0.907151460647583\n",
            " --Epoch 12\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 12 LR: [0.08264049551198992]\n",
            "  \tTrain Loss: 0.441 | Train Acc: 84.63%\n",
            "  \t Val. Loss: 0.391 |  Val Acc: 89.56%\n",
            "  Current best Val Acc: 0.907151460647583\n",
            " --Epoch 13\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 13 LR: [0.01424664832292858]\n",
            "  \tTrain Loss: 0.436 | Train Acc: 84.81%\n",
            "  \t Val. Loss: 0.359 |  Val Acc: 90.00%\n",
            "  Current best Val Acc: 0.907151460647583\n",
            " --Epoch 14\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 14 LR: [0.014829184220305288]\n",
            "  \tTrain Loss: 0.439 | Train Acc: 84.69%\n",
            "  \t Val. Loss: 0.351 |  Val Acc: 90.35%\n",
            "  Current best Val Acc: 0.907151460647583\n",
            " --Epoch 15\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 15 LR: [0.08273569243054962]\n",
            "  \tTrain Loss: 0.434 | Train Acc: 84.87%\n",
            "  \t Val. Loss: 0.362 |  Val Acc: 90.55%\n",
            "  Current best Val Acc: 0.907151460647583\n",
            " --Epoch 16\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 16 LR: [0.09122470424783691]\n",
            "  \tTrain Loss: 0.434 | Train Acc: 84.86%\n",
            "  \t Val. Loss: 0.378 |  Val Acc: 90.06%\n",
            "  Current best Val Acc: 0.907151460647583\n",
            " --Epoch 17\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 17 LR: [0.023935585672309027]\n",
            "  \tTrain Loss: 0.433 | Train Acc: 84.90%\n",
            "  \t Val. Loss: 0.363 |  Val Acc: 90.19%\n",
            "  Current best Val Acc: 0.907151460647583\n",
            " --Epoch 18\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 18 LR: [0.007940398285146248]\n",
            "  \tTrain Loss: 0.431 | Train Acc: 85.04%\n",
            "  \t Val. Loss: 0.362 |  Val Acc: 90.24%\n",
            "  Current best Val Acc: 0.907151460647583\n",
            " --Epoch 19\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 19 LR: [0.07218349773779938]\n",
            "  \tTrain Loss: 0.428 | Train Acc: 85.07%\n",
            "  \t Val. Loss: 0.346 |  Val Acc: 90.69%\n",
            "  Current best Val Acc: 0.907151460647583\n",
            " --Epoch 20\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 20 LR: [0.09726921269831688]\n",
            "  \tTrain Loss: 0.430 | Train Acc: 84.97%\n",
            "  \t Val. Loss: 0.382 |  Val Acc: 89.45%\n",
            "  Current best Val Acc: 0.907151460647583\n",
            " --Epoch 21\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 21 LR: [0.035294307809544685]\n",
            "  \tTrain Loss: 0.426 | Train Acc: 85.17%\n",
            "  \t Val. Loss: 0.360 |  Val Acc: 90.17%\n",
            "  Current best Val Acc: 0.907151460647583\n",
            " --Epoch 22\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 22 LR: [0.0008877413386836602]\n",
            "  \tTrain Loss: 0.425 | Train Acc: 85.11%\n",
            "  \t Val. Loss: 0.355 |  Val Acc: 90.53%\n",
            "  Current best Val Acc: 0.907151460647583\n",
            " --Epoch 23\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 23 LR: [0.060299677245396685]\n",
            "  \tTrain Loss: 0.425 | Train Acc: 85.16%\n",
            "  \t Val. Loss: 0.352 |  Val Acc: 90.01%\n",
            "  Current best Val Acc: 0.907151460647583\n",
            " --Epoch 24\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 24 LR: [0.10039542971065922]\n",
            "  \tTrain Loss: 0.422 | Train Acc: 85.28%\n",
            "  \t Val. Loss: 0.390 |  Val Acc: 89.76%\n",
            "  Current best Val Acc: 0.907151460647583\n",
            " --Epoch 25\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 25 LR: [0.047616082274993254]\n",
            "  \tTrain Loss: 0.424 | Train Acc: 85.21%\n",
            "  \t Val. Loss: 0.389 |  Val Acc: 90.15%\n",
            "  Current best Val Acc: 0.907151460647583\n",
            " --Epoch 26\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 26 LR: [0.00019885742750798052]\n",
            "  \tTrain Loss: 0.420 | Train Acc: 85.31%\n",
            "  \t Val. Loss: 0.357 |  Val Acc: 90.44%\n",
            "  Current best Val Acc: 0.907151460647583\n",
            " --Epoch 27\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 27 LR: [0.04783552847084498]\n",
            "  \tTrain Loss: 0.418 | Train Acc: 85.39%\n",
            "  \t Val. Loss: 0.345 |  Val Acc: 90.79%\n",
            "  Current best Val Acc: 0.9078525900840759\n",
            " --Epoch 28\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 28 LR: [0.10040810228446864]\n",
            "  \tTrain Loss: 0.416 | Train Acc: 85.51%\n",
            "  \t Val. Loss: 0.368 |  Val Acc: 90.14%\n",
            "  Current best Val Acc: 0.9078525900840759\n",
            " --Epoch 29\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 29 LR: [0.06013074866503443]\n",
            "  \tTrain Loss: 0.416 | Train Acc: 85.42%\n",
            "  \t Val. Loss: 0.398 |  Val Acc: 89.67%\n",
            "  Current best Val Acc: 0.9078525900840759\n",
            " --Epoch 30\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 30 LR: [0.0020913810196398304]\n",
            "  \tTrain Loss: 0.418 | Train Acc: 85.41%\n",
            "  \t Val. Loss: 0.338 |  Val Acc: 90.83%\n",
            "  Current best Val Acc: 0.9082531929016113\n",
            " --Epoch 31\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 31 LR: [0.03558241141589007]\n",
            "  \tTrain Loss: 0.416 | Train Acc: 85.43%\n",
            "  \t Val. Loss: 0.350 |  Val Acc: 90.94%\n",
            "  Current best Val Acc: 0.9093549847602844\n",
            " --Epoch 32\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 32 LR: [0.09730766144815217]\n",
            "  \tTrain Loss: 0.415 | Train Acc: 85.54%\n",
            "  \t Val. Loss: 0.348 |  Val Acc: 90.73%\n",
            "  Current best Val Acc: 0.9093549847602844\n",
            " --Epoch 33\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 33 LR: [0.07205464323828416]\n",
            "  \tTrain Loss: 0.413 | Train Acc: 85.60%\n",
            "  \t Val. Loss: 0.401 |  Val Acc: 89.48%\n",
            "  Current best Val Acc: 0.9093549847602844\n",
            " --Epoch 34\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 34 LR: [0.006822106297689017]\n",
            "  \tTrain Loss: 0.412 | Train Acc: 85.65%\n",
            "  \t Val. Loss: 0.335 |  Val Acc: 90.69%\n",
            "  Current best Val Acc: 0.9093549847602844\n",
            " --Epoch 35\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 35 LR: [0.024327804441606815]\n",
            "  \tTrain Loss: 0.409 | Train Acc: 85.77%\n",
            "  \t Val. Loss: 0.328 |  Val Acc: 91.04%\n",
            "  Current best Val Acc: 0.9103565812110901\n",
            " --Epoch 36\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 36 LR: [0.09129028427553074]\n",
            "  \tTrain Loss: 0.408 | Train Acc: 85.76%\n",
            "  \t Val. Loss: 0.340 |  Val Acc: 90.85%\n",
            "  Current best Val Acc: 0.9103565812110901\n",
            " --Epoch 37\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 37 LR: [0.08264049551231539]\n",
            "  \tTrain Loss: 0.407 | Train Acc: 85.87%\n",
            "  \t Val. Loss: 0.357 |  Val Acc: 90.30%\n",
            "  Current best Val Acc: 0.9103565812110901\n",
            " --Epoch 38\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 38 LR: [0.014246648322849621]\n",
            "  \tTrain Loss: 0.408 | Train Acc: 85.67%\n",
            "  \t Val. Loss: 0.337 |  Val Acc: 90.79%\n",
            "  Current best Val Acc: 0.9103565812110901\n",
            " --Epoch 39\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 39 LR: [0.0148291842204752]\n",
            "  \tTrain Loss: 0.405 | Train Acc: 85.90%\n",
            "  \t Val. Loss: 0.342 |  Val Acc: 90.99%\n",
            "  Current best Val Acc: 0.9103565812110901\n",
            " --Epoch 40\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 40 LR: [0.082735692430833]\n",
            "  \tTrain Loss: 0.404 | Train Acc: 85.94%\n",
            "  \t Val. Loss: 0.360 |  Val Acc: 90.64%\n",
            "  Current best Val Acc: 0.9103565812110901\n",
            " --Epoch 41\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 41 LR: [0.09122470424754144]\n",
            "  \tTrain Loss: 0.403 | Train Acc: 85.93%\n",
            "  \t Val. Loss: 0.421 |  Val Acc: 89.08%\n",
            "  Current best Val Acc: 0.9103565812110901\n",
            " --Epoch 42\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 42 LR: [0.02393558567279366]\n",
            "  \tTrain Loss: 0.402 | Train Acc: 85.97%\n",
            "  \t Val. Loss: 0.355 |  Val Acc: 90.60%\n",
            "  Current best Val Acc: 0.9103565812110901\n",
            " --Epoch 43\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 43 LR: [0.007940398285171403]\n",
            "  \tTrain Loss: 0.402 | Train Acc: 86.00%\n",
            "  \t Val. Loss: 0.354 |  Val Acc: 90.67%\n",
            "  Current best Val Acc: 0.9103565812110901\n",
            " --Epoch 44\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 44 LR: [0.07218349773693461]\n",
            "  \tTrain Loss: 0.402 | Train Acc: 85.99%\n",
            "  \t Val. Loss: 0.337 |  Val Acc: 90.69%\n",
            "  Current best Val Acc: 0.9103565812110901\n",
            " --Epoch 45\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 45 LR: [0.09726921269935908]\n",
            "  \tTrain Loss: 0.398 | Train Acc: 86.07%\n",
            "  \t Val. Loss: 0.371 |  Val Acc: 89.78%\n",
            "  Current best Val Acc: 0.9103565812110901\n",
            " --Epoch 46\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 46 LR: [0.035294307809923486]\n",
            "  \tTrain Loss: 0.397 | Train Acc: 86.26%\n",
            "  \t Val. Loss: 0.359 |  Val Acc: 90.22%\n",
            "  Current best Val Acc: 0.9103565812110901\n",
            " --Epoch 47\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 47 LR: [0.0008877413386836602]\n",
            "  \tTrain Loss: 0.399 | Train Acc: 86.10%\n",
            "  \t Val. Loss: 0.334 |  Val Acc: 90.69%\n",
            "  Current best Val Acc: 0.9103565812110901\n",
            " --Epoch 48\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 48 LR: [0.060299677245969685]\n",
            "  \tTrain Loss: 0.398 | Train Acc: 86.18%\n",
            "  \t Val. Loss: 0.336 |  Val Acc: 90.60%\n",
            "  Current best Val Acc: 0.9103565812110901\n",
            " --Epoch 49\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 49 LR: [0.10039542971032331]\n",
            "  \tTrain Loss: 0.397 | Train Acc: 86.17%\n",
            "  \t Val. Loss: 0.375 |  Val Acc: 89.97%\n",
            "  Current best Val Acc: 0.9103565812110901\n",
            " --Epoch 50\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 50 LR: [0.047616082274821364]\n",
            "  \tTrain Loss: 0.396 | Train Acc: 86.20%\n",
            "  \t Val. Loss: 0.362 |  Val Acc: 90.53%\n",
            "  Current best Val Acc: 0.9103565812110901\n",
            " --Epoch 51\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 51 LR: [0.0001988574275128639]\n",
            "  \tTrain Loss: 0.397 | Train Acc: 86.21%\n",
            "  \t Val. Loss: 0.337 |  Val Acc: 91.05%\n",
            "  Current best Val Acc: 0.9104567170143127\n",
            " --Epoch 52\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 52 LR: [0.047835528470858375]\n",
            "  \tTrain Loss: 0.394 | Train Acc: 86.25%\n",
            "  \t Val. Loss: 0.345 |  Val Acc: 90.99%\n",
            "  Current best Val Acc: 0.9104567170143127\n",
            " --Epoch 53\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 53 LR: [0.10040810228267923]\n",
            "  \tTrain Loss: 0.393 | Train Acc: 86.26%\n",
            "  \t Val. Loss: 0.353 |  Val Acc: 90.57%\n",
            "  Current best Val Acc: 0.9104567170143127\n",
            " --Epoch 54\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 54 LR: [0.06013074866573208]\n",
            "  \tTrain Loss: 0.392 | Train Acc: 86.43%\n",
            "  \t Val. Loss: 0.378 |  Val Acc: 90.33%\n",
            "  Current best Val Acc: 0.9104567170143127\n",
            " --Epoch 55\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 55 LR: [0.0020913810196302673]\n",
            "  \tTrain Loss: 0.392 | Train Acc: 86.32%\n",
            "  \t Val. Loss: 0.343 |  Val Acc: 91.05%\n",
            "  Current best Val Acc: 0.9104567170143127\n",
            " --Epoch 56\n",
            " --start training--\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "best_result_SGD = (0.1, 128, 0.0001)\n",
        "lr, batch_size, wd = best_result_SGD\n",
        "\n",
        "N_EPOCHS = 60\n",
        "\n",
        "print(\"--------------lr={}, batch_size={}, wd={} start-------------\".format(lr, batch_size, wd))\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "#T_max of lr_scheduler should be 1/2 or 1/3 of N_EPOCHES\n",
        "cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=25, eta_min=0.0001)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# use different transform applied to the dataset to create a bigger dataset for training\n",
        "transform1 = transforms.Compose([\n",
        "        \n",
        "        transforms.RandomResizedCrop(32),\n",
        "    transforms.RandomHorizontalFlip(p=1),\n",
        "        transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "transform2 = transforms.Compose([\n",
        "        \n",
        "        transforms.RandomResizedCrop(32),\n",
        "    transforms.RandomVerticalFlip(p=1),\n",
        "        transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "transform3 = transforms.Compose([\n",
        "        \n",
        "        transforms.RandomResizedCrop(32),\n",
        "    transforms.RandomRotation(90),\n",
        "        transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainset1 = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform1)\n",
        "trainset2 = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform2)\n",
        "trainset3 = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform3)\n",
        "\n",
        "trainset = ConcatDataset([trainset,trainset1,trainset3,trainset2])\n",
        "\n",
        "#test dataset should be leaved unchanged\n",
        "transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0,drop_last=True)\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0,drop_last=True)\n",
        "\n",
        "result = run_epoches(N_EPOCHS, model, optimizer, criterion, trainloader, testloader, cosine_scheduler)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "gem3ckGPx7uy",
        "outputId": "60e9d194-0a54-4490-8704-2125ef6bcdfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------lr=0.1, wd=0.0005, batch_size=128, start-------------\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "start running\n",
            " --Epoch 0\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 0 LR: [0.004894348370484647]\n",
            "  \tTrain Loss: 1.601 | Train Acc: 41.89%\n",
            "  \t Val. Loss: 1.218 |  Val Acc: 55.99%\n",
            "  Current best Val Acc: 0.559928834438324\n",
            " --Epoch 1\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 1 LR: [0.08386590697411969]\n",
            "  \tTrain Loss: 1.139 | Train Acc: 59.20%\n",
            "  \t Val. Loss: 2.041 |  Val Acc: 36.82%\n",
            "  Current best Val Acc: 0.559928834438324\n",
            " --Epoch 2\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 2 LR: [0.04448589487622449]\n",
            "  \tTrain Loss: 0.874 | Train Acc: 69.12%\n",
            "  \t Val. Loss: 0.813 |  Val Acc: 71.81%\n",
            "  Current best Val Acc: 0.7180577516555786\n",
            " --Epoch 3\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 3 LR: [0.05395961100810772]\n",
            "  \tTrain Loss: 0.700 | Train Acc: 75.52%\n",
            "  \t Val. Loss: 0.957 |  Val Acc: 67.35%\n",
            "  Current best Val Acc: 0.7180577516555786\n",
            " --Epoch 4\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 4 LR: [0.07236067977502762]\n",
            "  \tTrain Loss: 0.579 | Train Acc: 79.79%\n",
            "  \t Val. Loss: 0.700 |  Val Acc: 76.17%\n",
            "  Current best Val Acc: 0.7616693377494812\n",
            " --Epoch 5\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 5 LR: [0.02387287570313194]\n",
            "  \tTrain Loss: 0.489 | Train Acc: 83.02%\n",
            "  \t Val. Loss: 0.711 |  Val Acc: 75.90%\n",
            "  Current best Val Acc: 0.7616693377494812\n",
            " --Epoch 6\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 6 LR: [0.09629599990798779]\n",
            "  \tTrain Loss: 0.408 | Train Acc: 85.91%\n",
            "  \t Val. Loss: 0.914 |  Val Acc: 70.10%\n",
            "  Current best Val Acc: 0.7616693377494812\n",
            " --Epoch 7\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 7 LR: [0.0044242119720911514]\n",
            "  \tTrain Loss: 0.345 | Train Acc: 88.10%\n",
            "  \t Val. Loss: 0.727 |  Val Acc: 76.77%\n",
            "  Current best Val Acc: 0.7677017450332642\n",
            " --Epoch 8\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 8 LR: [0.10521243143695261]\n",
            "  \tTrain Loss: 0.283 | Train Acc: 90.29%\n",
            "  \t Val. Loss: 1.193 |  Val Acc: 65.73%\n",
            "  Current best Val Acc: 0.7677017450332642\n",
            " --Epoch 9\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 9 LR: [0.0]\n",
            "  \tTrain Loss: 0.241 | Train Acc: 91.74%\n",
            "  \t Val. Loss: 0.700 |  Val Acc: 78.59%\n",
            "  Current best Val Acc: 0.785897970199585\n",
            " --Epoch 10\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 10 LR: [0.09516553824443334]\n",
            "  \tTrain Loss: 0.196 | Train Acc: 93.36%\n",
            "  \t Val. Loss: 1.617 |  Val Acc: 63.44%\n",
            "  Current best Val Acc: 0.785897970199585\n",
            " --Epoch 11\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 11 LR: [0.03726186376262511]\n",
            "  \tTrain Loss: 0.163 | Train Acc: 94.51%\n",
            "  \t Val. Loss: 0.664 |  Val Acc: 80.75%\n",
            "  Current best Val Acc: 0.8074564933776855\n",
            " --Epoch 12\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 12 LR: [0.0696804401296078]\n",
            "  \tTrain Loss: 0.123 | Train Acc: 95.86%\n",
            "  \t Val. Loss: 1.527 |  Val Acc: 66.81%\n",
            "  Current best Val Acc: 0.8074564933776855\n",
            " --Epoch 13\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 13 LR: [0.057913686581790816]\n",
            "  \tTrain Loss: 0.107 | Train Acc: 96.52%\n",
            "  \t Val. Loss: 0.753 |  Val Acc: 79.46%\n",
            "  Current best Val Acc: 0.8074564933776855\n",
            " --Epoch 14\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 14 LR: [0.03819660112502122]\n",
            "  \tTrain Loss: 0.083 | Train Acc: 97.26%\n",
            "  \t Val. Loss: 0.852 |  Val Acc: 77.99%\n",
            "  Current best Val Acc: 0.8074564933776855\n",
            " --Epoch 15\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 15 LR: [0.08567627457815871]\n",
            "  \tTrain Loss: 0.077 | Train Acc: 97.40%\n",
            "  \t Val. Loss: 0.753 |  Val Acc: 80.66%\n",
            "  Current best Val Acc: 0.8074564933776855\n",
            " --Epoch 16\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 16 LR: [0.012295598939796277]\n",
            "  \tTrain Loss: 0.050 | Train Acc: 98.46%\n",
            "  \t Val. Loss: 0.774 |  Val Acc: 79.93%\n",
            "  Current best Val Acc: 0.8074564933776855\n",
            " --Epoch 17\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 17 LR: [0.10305368692687884]\n",
            "  \tTrain Loss: 0.040 | Train Acc: 98.78%\n",
            "  \t Val. Loss: 0.913 |  Val Acc: 77.97%\n",
            "  Current best Val Acc: 0.8074564933776855\n",
            " --Epoch 18\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 18 LR: [0.0006271407734226758]\n",
            "  \tTrain Loss: 0.036 | Train Acc: 98.93%\n",
            "  \t Val. Loss: 0.744 |  Val Acc: 82.11%\n",
            "  Current best Val Acc: 0.8211036324501038\n",
            " --Epoch 19\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 19 LR: [0.10250856309378148]\n",
            "  \tTrain Loss: 0.020 | Train Acc: 99.46%\n",
            "  \t Val. Loss: 0.760 |  Val Acc: 82.00%\n",
            "  Current best Val Acc: 0.8211036324501038\n",
            "--end running\n",
            "--------------lr=0.1, wd=0.0005, batch_size=128, result=0.8211036324501038-------------\n",
            "current best hyperparameters:(0.1, 128, 0.0005, tensor(0.8211, device='cuda:0'))\n",
            "--------------lr=0.1, wd=0.0005, batch_size=256, start-------------\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "start running\n",
            " --Epoch 0\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 0 LR: [0.08567627457812256]\n",
            "  \tTrain Loss: 1.675 | Train Acc: 39.53%\n",
            "  \t Val. Loss: 1.605 |  Val Acc: 41.93%\n",
            "  Current best Val Acc: 0.4193359315395355\n",
            " --Epoch 1\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 1 LR: [0.03726186376263812]\n",
            "  \tTrain Loss: 1.245 | Train Acc: 55.31%\n",
            "  \t Val. Loss: 1.044 |  Val Acc: 62.03%\n",
            "  Current best Val Acc: 0.620312511920929\n",
            " --Epoch 2\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 2 LR: [0.0044242119720878164]\n",
            "  \tTrain Loss: 0.988 | Train Acc: 64.80%\n",
            "  \t Val. Loss: 0.912 |  Val Acc: 67.49%\n",
            "  Current best Val Acc: 0.6749023795127869\n",
            " --Epoch 3\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 3 LR: [0.053959611008112265]\n",
            "  \tTrain Loss: 0.798 | Train Acc: 72.01%\n",
            "  \t Val. Loss: 2.064 |  Val Acc: 37.77%\n",
            "  Current best Val Acc: 0.6749023795127869\n",
            " --Epoch 4\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 4 LR: [0.10250856309367808]\n",
            "  \tTrain Loss: 0.661 | Train Acc: 76.85%\n",
            "  \t Val. Loss: 2.113 |  Val Acc: 44.88%\n",
            "  Current best Val Acc: 0.6749023795127869\n",
            " --Epoch 5\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 5 LR: [0.0856762745781386]\n",
            "  \tTrain Loss: 0.556 | Train Acc: 80.77%\n",
            "  \t Val. Loss: 0.774 |  Val Acc: 73.25%\n",
            "  Current best Val Acc: 0.7325195670127869\n",
            " --Epoch 6\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 6 LR: [0.03726186376265223]\n",
            "  \tTrain Loss: 0.459 | Train Acc: 84.04%\n",
            "  \t Val. Loss: 0.650 |  Val Acc: 77.52%\n",
            "  Current best Val Acc: 0.775195300579071\n",
            " --Epoch 7\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 7 LR: [0.004424211972089576]\n",
            "  \tTrain Loss: 0.379 | Train Acc: 86.93%\n",
            "  \t Val. Loss: 0.823 |  Val Acc: 73.54%\n",
            "  Current best Val Acc: 0.775195300579071\n",
            " --Epoch 8\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 8 LR: [0.053959611008103134]\n",
            "  \tTrain Loss: 0.305 | Train Acc: 89.61%\n",
            "  \t Val. Loss: 2.869 |  Val Acc: 38.38%\n",
            "  Current best Val Acc: 0.775195300579071\n",
            " --Epoch 9\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 9 LR: [0.10250856309373672]\n",
            "  \tTrain Loss: 0.263 | Train Acc: 91.05%\n",
            "  \t Val. Loss: 2.564 |  Val Acc: 48.35%\n",
            "  Current best Val Acc: 0.775195300579071\n",
            " --Epoch 10\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 10 LR: [0.08567627457808892]\n",
            "  \tTrain Loss: 0.206 | Train Acc: 93.11%\n",
            "  \t Val. Loss: 0.832 |  Val Acc: 74.89%\n",
            "  Current best Val Acc: 0.775195300579071\n",
            " --Epoch 11\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 11 LR: [0.037261863762673327]\n",
            "  \tTrain Loss: 0.163 | Train Acc: 94.66%\n",
            "  \t Val. Loss: 0.752 |  Val Acc: 78.04%\n",
            "  Current best Val Acc: 0.7803711295127869\n",
            " --Epoch 12\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 12 LR: [0.004424211972091609]\n",
            "  \tTrain Loss: 0.107 | Train Acc: 96.57%\n",
            "  \t Val. Loss: 0.935 |  Val Acc: 75.90%\n",
            "  Current best Val Acc: 0.7803711295127869\n",
            " --Epoch 13\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 13 LR: [0.053959611008107304]\n",
            "  \tTrain Loss: 0.084 | Train Acc: 97.40%\n",
            "  \t Val. Loss: 1.642 |  Val Acc: 66.33%\n",
            "  Current best Val Acc: 0.7803711295127869\n",
            " --Epoch 14\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 14 LR: [0.1025085630936854]\n",
            "  \tTrain Loss: 0.065 | Train Acc: 98.05%\n",
            "  \t Val. Loss: 1.519 |  Val Acc: 67.30%\n",
            "  Current best Val Acc: 0.7803711295127869\n",
            " --Epoch 15\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 15 LR: [0.0856762745781432]\n",
            "  \tTrain Loss: 0.078 | Train Acc: 97.50%\n",
            "  \t Val. Loss: 0.905 |  Val Acc: 77.00%\n",
            "  Current best Val Acc: 0.7803711295127869\n",
            " --Epoch 16\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 16 LR: [0.03726186376265534]\n",
            "  \tTrain Loss: 0.036 | Train Acc: 99.02%\n",
            "  \t Val. Loss: 0.831 |  Val Acc: 79.13%\n",
            "  Current best Val Acc: 0.791308581829071\n",
            " --Epoch 17\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 17 LR: [0.004424211972090067]\n",
            "  \tTrain Loss: 0.020 | Train Acc: 99.53%\n",
            "  \t Val. Loss: 0.897 |  Val Acc: 78.50%\n",
            "  Current best Val Acc: 0.791308581829071\n",
            " --Epoch 18\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 18 LR: [0.053959611008123354]\n",
            "  \tTrain Loss: 0.009 | Train Acc: 99.87%\n",
            "  \t Val. Loss: 0.828 |  Val Acc: 79.25%\n",
            "  Current best Val Acc: 0.79248046875\n",
            " --Epoch 19\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 19 LR: [0.1025085630937076]\n",
            "  \tTrain Loss: 0.003 | Train Acc: 99.99%\n",
            "  \t Val. Loss: 0.773 |  Val Acc: 80.77%\n",
            "  Current best Val Acc: 0.8077148795127869\n",
            "--end running\n",
            "--------------lr=0.1, wd=0.0005, batch_size=256, result=0.8077148795127869-------------\n",
            "current best hyperparameters:(0.1, 128, 0.0005, tensor(0.8211, device='cuda:0'))\n",
            "--------------lr=0.1, wd=0.0001, batch_size=128, start-------------\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "start running\n",
            " --Epoch 0\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 0 LR: [0.004894348370484647]\n",
            "  \tTrain Loss: 1.558 | Train Acc: 43.30%\n",
            "  \t Val. Loss: 1.158 |  Val Acc: 58.04%\n",
            "  Current best Val Acc: 0.5803995132446289\n",
            " --Epoch 1\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 1 LR: [0.08386590697411969]\n",
            "  \tTrain Loss: 1.035 | Train Acc: 63.16%\n",
            "  \t Val. Loss: 1.407 |  Val Acc: 53.08%\n",
            "  Current best Val Acc: 0.5803995132446289\n",
            " --Epoch 2\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 2 LR: [0.04448589487622449]\n",
            "  \tTrain Loss: 0.778 | Train Acc: 72.61%\n",
            "  \t Val. Loss: 0.734 |  Val Acc: 74.54%\n",
            "  Current best Val Acc: 0.7453520894050598\n",
            " --Epoch 3\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 3 LR: [0.05395961100810772]\n",
            "  \tTrain Loss: 0.633 | Train Acc: 77.94%\n",
            "  \t Val. Loss: 1.093 |  Val Acc: 65.21%\n",
            "  Current best Val Acc: 0.7453520894050598\n",
            " --Epoch 4\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 4 LR: [0.07236067977502762]\n",
            "  \tTrain Loss: 0.526 | Train Acc: 81.84%\n",
            "  \t Val. Loss: 0.651 |  Val Acc: 77.68%\n",
            "  Current best Val Acc: 0.7767998576164246\n",
            " --Epoch 5\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 5 LR: [0.02387287570313194]\n",
            "  \tTrain Loss: 0.437 | Train Acc: 84.77%\n",
            "  \t Val. Loss: 0.886 |  Val Acc: 72.31%\n",
            "  Current best Val Acc: 0.7767998576164246\n",
            " --Epoch 6\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 6 LR: [0.09629599990798779]\n",
            "  \tTrain Loss: 0.369 | Train Acc: 87.38%\n",
            "  \t Val. Loss: 0.735 |  Val Acc: 76.02%\n",
            "  Current best Val Acc: 0.7767998576164246\n",
            " --Epoch 7\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 7 LR: [0.0044242119720911514]\n",
            "  \tTrain Loss: 0.303 | Train Acc: 89.54%\n",
            "  \t Val. Loss: 0.767 |  Val Acc: 77.17%\n",
            "  Current best Val Acc: 0.7767998576164246\n",
            " --Epoch 8\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 8 LR: [0.10521243143695261]\n",
            "  \tTrain Loss: 0.243 | Train Acc: 91.57%\n",
            "  \t Val. Loss: 0.877 |  Val Acc: 74.76%\n",
            "  Current best Val Acc: 0.7767998576164246\n",
            " --Epoch 9\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 9 LR: [0.0]\n",
            "  \tTrain Loss: 0.200 | Train Acc: 93.08%\n",
            "  \t Val. Loss: 0.693 |  Val Acc: 79.45%\n",
            "  Current best Val Acc: 0.7945016026496887\n",
            " --Epoch 10\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 10 LR: [0.09516553824443334]\n",
            "  \tTrain Loss: 0.156 | Train Acc: 94.62%\n",
            "  \t Val. Loss: 1.769 |  Val Acc: 63.82%\n",
            "  Current best Val Acc: 0.7945016026496887\n",
            " --Epoch 11\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 11 LR: [0.03726186376262511]\n",
            "  \tTrain Loss: 0.147 | Train Acc: 94.92%\n",
            "  \t Val. Loss: 0.697 |  Val Acc: 79.46%\n",
            "  Current best Val Acc: 0.7946004867553711\n",
            " --Epoch 12\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 12 LR: [0.0696804401296078]\n",
            "  \tTrain Loss: 0.091 | Train Acc: 96.93%\n",
            "  \t Val. Loss: 0.956 |  Val Acc: 76.07%\n",
            "  Current best Val Acc: 0.7946004867553711\n",
            " --Epoch 13\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 13 LR: [0.057913686581790816]\n",
            "  \tTrain Loss: 0.069 | Train Acc: 97.76%\n",
            "  \t Val. Loss: 0.781 |  Val Acc: 80.01%\n",
            "  Current best Val Acc: 0.8001384735107422\n",
            " --Epoch 14\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 14 LR: [0.03819660112502122]\n",
            "  \tTrain Loss: 0.056 | Train Acc: 98.22%\n",
            "  \t Val. Loss: 1.129 |  Val Acc: 74.71%\n",
            "  Current best Val Acc: 0.8001384735107422\n",
            " --Epoch 15\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 15 LR: [0.08567627457815871]\n",
            "  \tTrain Loss: 0.052 | Train Acc: 98.30%\n",
            "  \t Val. Loss: 0.848 |  Val Acc: 79.67%\n",
            "  Current best Val Acc: 0.8001384735107422\n",
            " --Epoch 16\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 16 LR: [0.012295598939796277]\n",
            "  \tTrain Loss: 0.049 | Train Acc: 98.37%\n",
            "  \t Val. Loss: 0.884 |  Val Acc: 79.59%\n",
            "  Current best Val Acc: 0.8001384735107422\n",
            " --Epoch 17\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 17 LR: [0.10305368692687884]\n",
            "  \tTrain Loss: 0.044 | Train Acc: 98.60%\n",
            "  \t Val. Loss: 0.875 |  Val Acc: 79.78%\n",
            "  Current best Val Acc: 0.8001384735107422\n",
            " --Epoch 18\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 18 LR: [0.0006271407734226758]\n",
            "  \tTrain Loss: 0.048 | Train Acc: 98.39%\n",
            "  \t Val. Loss: 0.848 |  Val Acc: 80.43%\n",
            "  Current best Val Acc: 0.8042919635772705\n",
            " --Epoch 19\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 19 LR: [0.10250856309378148]\n",
            "  \tTrain Loss: 0.031 | Train Acc: 98.98%\n",
            "  \t Val. Loss: 0.888 |  Val Acc: 80.87%\n",
            "  Current best Val Acc: 0.808742105960846\n",
            "--end running\n",
            "--------------lr=0.1, wd=0.0001, batch_size=128, result=0.808742105960846-------------\n",
            "current best hyperparameters:(0.1, 128, 0.0005, tensor(0.8211, device='cuda:0'))\n",
            "--------------lr=0.1, wd=0.0001, batch_size=256, start-------------\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "start running\n",
            " --Epoch 0\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 0 LR: [0.08567627457812256]\n",
            "  \tTrain Loss: 1.681 | Train Acc: 38.65%\n",
            "  \t Val. Loss: 1.364 |  Val Acc: 49.05%\n",
            "  Current best Val Acc: 0.49052736163139343\n",
            " --Epoch 1\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 1 LR: [0.03726186376263812]\n",
            "  \tTrain Loss: 1.269 | Train Acc: 53.91%\n",
            "  \t Val. Loss: 1.106 |  Val Acc: 60.22%\n",
            "  Current best Val Acc: 0.602246105670929\n",
            " --Epoch 2\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 2 LR: [0.0044242119720878164]\n",
            "  \tTrain Loss: 1.036 | Train Acc: 62.70%\n",
            "  \t Val. Loss: 1.070 |  Val Acc: 61.88%\n",
            "  Current best Val Acc: 0.618847668170929\n",
            " --Epoch 3\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 3 LR: [0.053959611008112265]\n",
            "  \tTrain Loss: 0.826 | Train Acc: 70.92%\n",
            "  \t Val. Loss: 1.567 |  Val Acc: 49.02%\n",
            "  Current best Val Acc: 0.618847668170929\n",
            " --Epoch 4\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 4 LR: [0.10250856309367808]\n",
            "  \tTrain Loss: 0.686 | Train Acc: 76.23%\n",
            "  \t Val. Loss: 1.605 |  Val Acc: 51.71%\n",
            "  Current best Val Acc: 0.618847668170929\n",
            " --Epoch 5\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 5 LR: [0.0856762745781386]\n",
            "  \tTrain Loss: 0.577 | Train Acc: 79.81%\n",
            "  \t Val. Loss: 0.744 |  Val Acc: 74.23%\n",
            "  Current best Val Acc: 0.7422851920127869\n",
            " --Epoch 6\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 6 LR: [0.03726186376265223]\n",
            "  \tTrain Loss: 0.485 | Train Acc: 83.32%\n",
            "  \t Val. Loss: 0.639 |  Val Acc: 78.26%\n",
            "  Current best Val Acc: 0.7826172113418579\n",
            " --Epoch 7\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 7 LR: [0.004424211972089576]\n",
            "  \tTrain Loss: 0.402 | Train Acc: 86.23%\n",
            "  \t Val. Loss: 0.708 |  Val Acc: 76.28%\n",
            "  Current best Val Acc: 0.7826172113418579\n",
            " --Epoch 8\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 8 LR: [0.053959611008103134]\n",
            "  \tTrain Loss: 0.325 | Train Acc: 88.96%\n",
            "  \t Val. Loss: 1.402 |  Val Acc: 61.88%\n",
            "  Current best Val Acc: 0.7826172113418579\n",
            " --Epoch 9\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 9 LR: [0.10250856309373672]\n",
            "  \tTrain Loss: 0.267 | Train Acc: 91.02%\n",
            "  \t Val. Loss: 3.744 |  Val Acc: 34.00%\n",
            "  Current best Val Acc: 0.7826172113418579\n",
            " --Epoch 10\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 10 LR: [0.08567627457808892]\n",
            "  \tTrain Loss: 0.212 | Train Acc: 92.86%\n",
            "  \t Val. Loss: 0.884 |  Val Acc: 73.63%\n",
            "  Current best Val Acc: 0.7826172113418579\n",
            " --Epoch 11\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 11 LR: [0.037261863762673327]\n",
            "  \tTrain Loss: 0.162 | Train Acc: 94.68%\n",
            "  \t Val. Loss: 0.708 |  Val Acc: 79.52%\n",
            "  Current best Val Acc: 0.795214831829071\n",
            " --Epoch 12\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 12 LR: [0.004424211972091609]\n",
            "  \tTrain Loss: 0.114 | Train Acc: 96.36%\n",
            "  \t Val. Loss: 0.828 |  Val Acc: 77.17%\n",
            "  Current best Val Acc: 0.795214831829071\n",
            " --Epoch 13\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 13 LR: [0.053959611008107304]\n",
            "  \tTrain Loss: 0.084 | Train Acc: 97.36%\n",
            "  \t Val. Loss: 1.623 |  Val Acc: 65.32%\n",
            "  Current best Val Acc: 0.795214831829071\n",
            " --Epoch 14\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 14 LR: [0.1025085630936854]\n",
            "  \tTrain Loss: 0.065 | Train Acc: 98.03%\n",
            "  \t Val. Loss: 1.119 |  Val Acc: 74.09%\n",
            "  Current best Val Acc: 0.795214831829071\n",
            " --Epoch 15\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 15 LR: [0.0856762745781432]\n",
            "  \tTrain Loss: 0.042 | Train Acc: 98.83%\n",
            "  \t Val. Loss: 1.073 |  Val Acc: 74.91%\n",
            "  Current best Val Acc: 0.795214831829071\n",
            " --Epoch 16\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 16 LR: [0.03726186376265534]\n",
            "  \tTrain Loss: 0.016 | Train Acc: 99.73%\n",
            "  \t Val. Loss: 0.817 |  Val Acc: 80.34%\n",
            "  Current best Val Acc: 0.803417980670929\n",
            " --Epoch 17\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 17 LR: [0.004424211972090067]\n",
            "  \tTrain Loss: 0.007 | Train Acc: 99.93%\n",
            "  \t Val. Loss: 0.826 |  Val Acc: 80.50%\n",
            "  Current best Val Acc: 0.804980456829071\n",
            " --Epoch 18\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 18 LR: [0.053959611008123354]\n",
            "  \tTrain Loss: 0.003 | Train Acc: 100.00%\n",
            "  \t Val. Loss: 0.870 |  Val Acc: 79.39%\n",
            "  Current best Val Acc: 0.804980456829071\n",
            " --Epoch 19\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 19 LR: [0.1025085630937076]\n",
            "  \tTrain Loss: 0.002 | Train Acc: 99.99%\n",
            "  \t Val. Loss: 0.807 |  Val Acc: 81.35%\n",
            "  Current best Val Acc: 0.8134765625\n",
            "--end running\n",
            "--------------lr=0.1, wd=0.0001, batch_size=256, result=0.8134765625-------------\n",
            "current best hyperparameters:(0.1, 128, 0.0005, tensor(0.8211, device='cuda:0'))\n",
            "--------------lr=0.05, wd=0.0005, batch_size=128, start-------------\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "start running\n",
            " --Epoch 0\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 0 LR: [0.0024471741852423235]\n",
            "  \tTrain Loss: 1.484 | Train Acc: 45.48%\n",
            "  \t Val. Loss: 1.139 |  Val Acc: 58.56%\n",
            "  Current best Val Acc: 0.5856408476829529\n",
            " --Epoch 1\n",
            " --start training--\n"
          ]
        }
      ],
      "source": [
        "# This code is not working for the final result. It is my exhaustive search for some hyperparameters that may help better result.\n",
        "# Therefore, this block need not to run any more.\n",
        "\n",
        "N_EPOCHS = 20\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "lr_candidates = [0.1, 0.05, 0.005]\n",
        "wd_candidates = [5e-4,1e-4]\n",
        "\n",
        "batch_size_candidates = [128, 256]\n",
        "\n",
        "best_result = (0,0,0,0)\n",
        "for lr in lr_candidates:\n",
        "  for wd in wd_candidates:\n",
        "    for batch_size in batch_size_candidates:\n",
        "      print(\"--------------lr={}, wd={}, batch_size={}, start-------------\".format(lr, wd, batch_size))\n",
        "      # optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "      model = ResNet(BuildingBlock, [3,3,2,2], 3, 32, 10)\n",
        "      model = model.to(device)\n",
        "      optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=wd)\n",
        "      cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
        "\n",
        "      trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "      trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "      testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "      testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "      result = run_epoches(N_EPOCHS, model, optimizer,  criterion, trainloader, testloader， cosine_scheduler)\n",
        "      print(\"--------------lr={}, wd={}, batch_size={}, result={}-------------\".format(lr, wd, batch_size, result))\n",
        "      if result > best_result[3]:\n",
        "        best_result = (lr, batch_size, wd, result)\n",
        "      print('current best hyperparameters:{}'.format(best_result))\n",
        "    \n",
        "print(best_result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}