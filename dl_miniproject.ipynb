{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DemoySegment/dl-miniproject-resnet/blob/main/dl_miniproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDPCtreRsDjn",
        "outputId": "78cf589b-0172-41e0-97e7-df56e3e71b3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.9/dist-packages (1.7.2)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchsummary\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as functional\n",
        "import torchvision.models as models\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import ConcatDataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "G8HqCmimo_HB"
      },
      "outputs": [],
      "source": [
        "# output_size = (input_size + 2*padding - kernel)/stride + 1 \n",
        "class BuildingBlock(nn.Module):\n",
        "   \n",
        "    def __init__(self, in_channels, intermediate_channels, identity_downsample=None, kernel_size=3, stride=1, expansion=1):\n",
        "      \"\"\"\n",
        "      This class is for building a resnet block. In each block various \n",
        "      convolution layers will be connected to each other with a batctnorm layer and a relu activation between.\n",
        "      Skip connection will be built between the input of the first layer and the input of the last layer, \n",
        "      that is to add the input of the block to the output of the last batctnorm layer.\n",
        "      Size of the two inputs of skip connection should be pay attention to.\n",
        "\n",
        "      :param in_channels: the number of input channels of the whole block. Since block will repeat several times, \n",
        "      lets say a block with with a input channels of 64 and output channels of 128, the next time going\n",
        "      through the block need a input channels of 128.\n",
        "\n",
        "      :param intermediate_channels: the number of output channels of conv layers in the block. \n",
        "      Since channels always expand, the output channels of the block will be the expansion * intermediate_channels.\n",
        "\n",
        "      :param identity_downsample: a model to deal with skip connection problem. this model should have a conv layer and\n",
        "      a batchnorm layer. In the next iteration of same block, the input channels may not be consist with the output of the \n",
        "      last batchnorm output, therefore we need the parameter to help change x's channels.\n",
        "      :type identity_downsample: nn.Module\n",
        "\n",
        "      :paran stride: if stride>1 for one conv layer in each same block in iteration, then the size of the images will be \n",
        "      decreased for block_num times, which is not what we want. Therefore, for iterations of the the same block, only one layer\n",
        "      in one of the block will have a stride that reduce the size of the image.\n",
        "      \"\"\"\n",
        "\n",
        "      super(BuildingBlock, self).__init__()\n",
        "      \n",
        "      #expansion rate, the output channels of the block will be the expansion * intermediate_channels\n",
        "      self.expansion = expansion\n",
        "      self.conv1 = nn.Conv2d(\n",
        "          in_channels,\n",
        "          intermediate_channels,\n",
        "          kernel_size=kernel_size,\n",
        "          stride=stride,\n",
        "          padding=(int)((kernel_size-1)/2),\n",
        "          bias=False,\n",
        "      )\n",
        "      self.bn1 = nn.BatchNorm2d(intermediate_channels)\n",
        "      self.conv2 = nn.Conv2d(\n",
        "          intermediate_channels,\n",
        "          intermediate_channels * self.expansion,\n",
        "          kernel_size=kernel_size,\n",
        "          stride=1,\n",
        "          padding=(int)((kernel_size-1)/2),\n",
        "          bias=False,\n",
        "      )\n",
        "      self.bn2 = nn.BatchNorm2d(intermediate_channels * self.expansion)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.identity_downsample = identity_downsample\n",
        "      self.stride = stride\n",
        "      self.in_channels = in_channels\n",
        "      self.intermediate_channels = intermediate_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(identity)\n",
        "\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layerNums, image_channels, start_channels, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "        # head layers\n",
        "        self.in_channels = start_channels\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            image_channels, self.in_channels, kernel_size=3, stride=1, padding=1, bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # recursion block layers\n",
        "        # Essentially the entire ResNet architecture are in these 4 lines below\n",
        "        self.layer1, self.in_channels = self._make_block(\n",
        "            BuildingBlock, layerNums[0], intermediate_channels=32, in_channels=self.in_channels, stride=1, kernel_size=5\n",
        "        )\n",
        "        self.layer2, self.in_channels = self._make_block(\n",
        "            BuildingBlock, layerNums[1], intermediate_channels=64, in_channels=self.in_channels, stride=2, kernel_size=5\n",
        "        )\n",
        "        self.layer3, self.in_channels = self._make_block(\n",
        "            BuildingBlock, layerNums[2], intermediate_channels=128, in_channels=self.in_channels, stride=2, kernel_size=5\n",
        "        )\n",
        "        self.layer4, self.in_channels = self._make_block(\n",
        "            BuildingBlock, layerNums[3], intermediate_channels=256, in_channels=self.in_channels, stride=2, kernel_size=3\n",
        "        )\n",
        "\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "        self.layerNums = layerNums\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        #x = self.maxpool(x)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        #x = functional.softmax(x, dim=0)\n",
        "        return x\n",
        "\n",
        "    # for resnet18, expansion === 1\n",
        "    def _make_block(self, block, num_layers, intermediate_channels, in_channels, stride, expansion=1, kernel_size=3):\n",
        "        identity_downsample = None\n",
        "        layers = []\n",
        "\n",
        "        # Either if we half the input space for ex, 56x56 -> 28x28 (stride=2), or channels changes\n",
        "        # we need to adapt the Identity (skip connection) so it will be able to be added\n",
        "        # to the layer that's ahead\n",
        "        # it is used at the end of first iteration of each block\n",
        "        if stride != 1 or in_channels != intermediate_channels*expansion:\n",
        "          identity_downsample = nn.Sequential(\n",
        "                  nn.Conv2d(\n",
        "                      in_channels,\n",
        "                      intermediate_channels*expansion,\n",
        "                      kernel_size=1,\n",
        "                      stride=stride,\n",
        "                      bias=False,\n",
        "                  ),\n",
        "                  nn.BatchNorm2d(intermediate_channels*expansion),\n",
        "              )\n",
        "\n",
        "        layers.append(\n",
        "            block(in_channels, intermediate_channels, stride=stride, identity_downsample=identity_downsample, expansion=expansion, kernel_size=kernel_size)\n",
        "        )\n",
        "\n",
        "       \n",
        "        in_channels = intermediate_channels*expansion\n",
        "\n",
        "        # For example for first resnet layer: 256 will be mapped to 64 as intermediate layer,\n",
        "        # then finally back to 256. Hence no identity downsample is needed, since stride = 1,\n",
        "        # and also same amount of channels.\n",
        "        for i in range(num_layers - 1):\n",
        "            layers.append(block(in_channels, intermediate_channels, expansion=expansion, kernel_size=kernel_size))\n",
        "        \n",
        "        return nn.Sequential(*layers), in_channels\n",
        "    \n",
        "    def to_string(self):\n",
        "      print('current model status:')\n",
        "      print('parameter numbers: {}'.format(sum(p.numel() for p in self.parameters() if p.requires_grad)))\n",
        "      print('block numbers: {}'.format(self.layerNums))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8YnQcc6pEaG",
        "outputId": "d8027631-cec2-4c99-a287-4f8efc086559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU()\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BuildingBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (1): BuildingBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (2): BuildingBlock(\n",
            "      (conv1): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BuildingBlock(\n",
            "      (conv1): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "      (identity_downsample): Sequential(\n",
            "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BuildingBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "    (2): BuildingBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BuildingBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "      (identity_downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BuildingBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BuildingBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "      (identity_downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BuildingBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ResNet(BuildingBlock, [3,3,2,2], 3, 32, 10)\n",
        "model = model.to(device)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ebq9KSyk7HeK",
        "outputId": "50767e14-b9dc-45fd-f8db-d7a7523d8682"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "ResNet                                   [256, 10]                 --\n",
              "├─Conv2d: 1-1                            [256, 32, 32, 32]         864\n",
              "├─BatchNorm2d: 1-2                       [256, 32, 32, 32]         64\n",
              "├─ReLU: 1-3                              [256, 32, 32, 32]         --\n",
              "├─Sequential: 1-4                        [256, 32, 32, 32]         --\n",
              "│    └─BuildingBlock: 2-1                [256, 32, 32, 32]         --\n",
              "│    │    └─Conv2d: 3-1                  [256, 32, 32, 32]         25,600\n",
              "│    │    └─BatchNorm2d: 3-2             [256, 32, 32, 32]         64\n",
              "│    │    └─ReLU: 3-3                    [256, 32, 32, 32]         --\n",
              "│    │    └─Conv2d: 3-4                  [256, 32, 32, 32]         25,600\n",
              "│    │    └─BatchNorm2d: 3-5             [256, 32, 32, 32]         64\n",
              "│    │    └─ReLU: 3-6                    [256, 32, 32, 32]         --\n",
              "│    └─BuildingBlock: 2-2                [256, 32, 32, 32]         --\n",
              "│    │    └─Conv2d: 3-7                  [256, 32, 32, 32]         25,600\n",
              "│    │    └─BatchNorm2d: 3-8             [256, 32, 32, 32]         64\n",
              "│    │    └─ReLU: 3-9                    [256, 32, 32, 32]         --\n",
              "│    │    └─Conv2d: 3-10                 [256, 32, 32, 32]         25,600\n",
              "│    │    └─BatchNorm2d: 3-11            [256, 32, 32, 32]         64\n",
              "│    │    └─ReLU: 3-12                   [256, 32, 32, 32]         --\n",
              "│    └─BuildingBlock: 2-3                [256, 32, 32, 32]         --\n",
              "│    │    └─Conv2d: 3-13                 [256, 32, 32, 32]         25,600\n",
              "│    │    └─BatchNorm2d: 3-14            [256, 32, 32, 32]         64\n",
              "│    │    └─ReLU: 3-15                   [256, 32, 32, 32]         --\n",
              "│    │    └─Conv2d: 3-16                 [256, 32, 32, 32]         25,600\n",
              "│    │    └─BatchNorm2d: 3-17            [256, 32, 32, 32]         64\n",
              "│    │    └─ReLU: 3-18                   [256, 32, 32, 32]         --\n",
              "├─Sequential: 1-5                        [256, 64, 16, 16]         --\n",
              "│    └─BuildingBlock: 2-4                [256, 64, 16, 16]         --\n",
              "│    │    └─Conv2d: 3-19                 [256, 64, 16, 16]         51,200\n",
              "│    │    └─BatchNorm2d: 3-20            [256, 64, 16, 16]         128\n",
              "│    │    └─ReLU: 3-21                   [256, 64, 16, 16]         --\n",
              "│    │    └─Conv2d: 3-22                 [256, 64, 16, 16]         102,400\n",
              "│    │    └─BatchNorm2d: 3-23            [256, 64, 16, 16]         128\n",
              "│    │    └─Sequential: 3-24             [256, 64, 16, 16]         2,176\n",
              "│    │    └─ReLU: 3-25                   [256, 64, 16, 16]         --\n",
              "│    └─BuildingBlock: 2-5                [256, 64, 16, 16]         --\n",
              "│    │    └─Conv2d: 3-26                 [256, 64, 16, 16]         102,400\n",
              "│    │    └─BatchNorm2d: 3-27            [256, 64, 16, 16]         128\n",
              "│    │    └─ReLU: 3-28                   [256, 64, 16, 16]         --\n",
              "│    │    └─Conv2d: 3-29                 [256, 64, 16, 16]         102,400\n",
              "│    │    └─BatchNorm2d: 3-30            [256, 64, 16, 16]         128\n",
              "│    │    └─ReLU: 3-31                   [256, 64, 16, 16]         --\n",
              "│    └─BuildingBlock: 2-6                [256, 64, 16, 16]         --\n",
              "│    │    └─Conv2d: 3-32                 [256, 64, 16, 16]         102,400\n",
              "│    │    └─BatchNorm2d: 3-33            [256, 64, 16, 16]         128\n",
              "│    │    └─ReLU: 3-34                   [256, 64, 16, 16]         --\n",
              "│    │    └─Conv2d: 3-35                 [256, 64, 16, 16]         102,400\n",
              "│    │    └─BatchNorm2d: 3-36            [256, 64, 16, 16]         128\n",
              "│    │    └─ReLU: 3-37                   [256, 64, 16, 16]         --\n",
              "├─Sequential: 1-6                        [256, 128, 8, 8]          --\n",
              "│    └─BuildingBlock: 2-7                [256, 128, 8, 8]          --\n",
              "│    │    └─Conv2d: 3-38                 [256, 128, 8, 8]          204,800\n",
              "│    │    └─BatchNorm2d: 3-39            [256, 128, 8, 8]          256\n",
              "│    │    └─ReLU: 3-40                   [256, 128, 8, 8]          --\n",
              "│    │    └─Conv2d: 3-41                 [256, 128, 8, 8]          409,600\n",
              "│    │    └─BatchNorm2d: 3-42            [256, 128, 8, 8]          256\n",
              "│    │    └─Sequential: 3-43             [256, 128, 8, 8]          8,448\n",
              "│    │    └─ReLU: 3-44                   [256, 128, 8, 8]          --\n",
              "│    └─BuildingBlock: 2-8                [256, 128, 8, 8]          --\n",
              "│    │    └─Conv2d: 3-45                 [256, 128, 8, 8]          409,600\n",
              "│    │    └─BatchNorm2d: 3-46            [256, 128, 8, 8]          256\n",
              "│    │    └─ReLU: 3-47                   [256, 128, 8, 8]          --\n",
              "│    │    └─Conv2d: 3-48                 [256, 128, 8, 8]          409,600\n",
              "│    │    └─BatchNorm2d: 3-49            [256, 128, 8, 8]          256\n",
              "│    │    └─ReLU: 3-50                   [256, 128, 8, 8]          --\n",
              "├─Sequential: 1-7                        [256, 256, 4, 4]          --\n",
              "│    └─BuildingBlock: 2-9                [256, 256, 4, 4]          --\n",
              "│    │    └─Conv2d: 3-51                 [256, 256, 4, 4]          294,912\n",
              "│    │    └─BatchNorm2d: 3-52            [256, 256, 4, 4]          512\n",
              "│    │    └─ReLU: 3-53                   [256, 256, 4, 4]          --\n",
              "│    │    └─Conv2d: 3-54                 [256, 256, 4, 4]          589,824\n",
              "│    │    └─BatchNorm2d: 3-55            [256, 256, 4, 4]          512\n",
              "│    │    └─Sequential: 3-56             [256, 256, 4, 4]          33,280\n",
              "│    │    └─ReLU: 3-57                   [256, 256, 4, 4]          --\n",
              "│    └─BuildingBlock: 2-10               [256, 256, 4, 4]          --\n",
              "│    │    └─Conv2d: 3-58                 [256, 256, 4, 4]          589,824\n",
              "│    │    └─BatchNorm2d: 3-59            [256, 256, 4, 4]          512\n",
              "│    │    └─ReLU: 3-60                   [256, 256, 4, 4]          --\n",
              "│    │    └─Conv2d: 3-61                 [256, 256, 4, 4]          589,824\n",
              "│    │    └─BatchNorm2d: 3-62            [256, 256, 4, 4]          512\n",
              "│    │    └─ReLU: 3-63                   [256, 256, 4, 4]          --\n",
              "├─AdaptiveAvgPool2d: 1-8                 [256, 256, 1, 1]          --\n",
              "├─Linear: 1-9                            [256, 10]                 2,570\n",
              "==========================================================================================\n",
              "Total params: 4,266,410\n",
              "Trainable params: 4,266,410\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 109.75\n",
              "==========================================================================================\n",
              "Input size (MB): 3.15\n",
              "Forward/backward pass size (MB): 1660.96\n",
              "Params size (MB): 17.07\n",
              "Estimated Total Size (MB): 1681.18\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "summary(model, input_size=(256, 3, 32, 32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yMYnIkQ2xGU5"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_pred, y):\n",
        "  predict = torch.argmax(y_pred, dim=1)\n",
        "  acc = torch.sum(predict == y) / y.shape[0]\n",
        "  return acc\n",
        "  #return y_pred.argmax(dim=1).eq(y).sum().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GpXNbq0uxR5j"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    #set the model in evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for(x, y) in iterator:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        y_pred = model(x)\n",
        "        loss = criterion(y_pred, y)\n",
        "        acc = accuracy(y_pred, y)\n",
        "        \n",
        "        epoch_loss += loss\n",
        "        epoch_acc += acc\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8i2h32KQw5d4"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, scheduler):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    #set the model in training mode\n",
        "    model.train()\n",
        "\n",
        "    for(x, y) in iterator:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "\n",
        "      y_pred = model(x)\n",
        "      \n",
        "      loss = criterion(y_pred, y)\n",
        "      acc = accuracy(y_pred, y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if scheduler is not None:\n",
        "        scheduler.step()\n",
        "      \n",
        "      epoch_loss += loss\n",
        "      epoch_acc += acc\n",
        "      \n",
        "\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DiLeiZtdgn8x"
      },
      "outputs": [],
      "source": [
        "def run_epoches(epoch_num, model, optimizer, criterion, trainloader, testloader, scheduler=None):\n",
        "  best_valid_acc = float(0)\n",
        "  print(\"start running\")\n",
        "  for epoch in range(N_EPOCHS):\n",
        "      print(' --Epoch {}'.format(epoch))\n",
        "      print(\" --start training--\")\n",
        "      train_loss, train_acc = train(model, trainloader, optimizer, criterion, scheduler)\n",
        "      print(\" --start validing--\")\n",
        "      valid_loss, valid_acc = evaluate(model, testloader, criterion)\n",
        "      if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "\n",
        "      print('Epoch:', epoch, 'LR:', scheduler.get_lr())\n",
        "      print(f'  \\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "      print(f'  \\t Val. Loss: {valid_loss:.3f} |  Val Acc: {valid_acc*100:.2f}%')\n",
        "      print(f'  Current best Val Acc: {best_valid_acc}')\n",
        "      torch.cuda.empty_cache()\n",
        "  print(\"--end running\")\n",
        "  return best_valid_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tYM-HUdKxcAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3b20683-ec92-4424-abb4-986e2d822852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------lr=0.1, batch_size=128, wd=0.0005 start-------------\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "start running\n",
            " --Epoch 0\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 0 LR: [0.08386590697411314]\n",
            "  \tTrain Loss: 0.627 | Train Acc: 78.11%\n",
            "  \t Val. Loss: 0.464 |  Val Acc: 84.78%\n",
            "  Current best Val Acc: 0.8477564454078674\n",
            " --Epoch 1\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 1 LR: [0.053959611008137134]\n",
            "  \tTrain Loss: 0.618 | Train Acc: 78.50%\n",
            "  \t Val. Loss: 0.409 |  Val Acc: 86.83%\n",
            "  Current best Val Acc: 0.8682892918586731\n",
            " --Epoch 2\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 2 LR: [0.023872875703126455]\n",
            "  \tTrain Loss: 0.612 | Train Acc: 78.73%\n",
            "  \t Val. Loss: 0.373 |  Val Acc: 87.84%\n",
            "  Current best Val Acc: 0.8784054517745972\n",
            " --Epoch 3\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 3 LR: [0.004424211972086993]\n",
            "  \tTrain Loss: 0.605 | Train Acc: 78.95%\n",
            "  \t Val. Loss: 0.376 |  Val Acc: 87.90%\n",
            "  Current best Val Acc: 0.8790064454078674\n",
            " --Epoch 4\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 4 LR: [0.0]\n",
            "  \tTrain Loss: 0.598 | Train Acc: 79.21%\n",
            "  \t Val. Loss: 0.342 |  Val Acc: 88.78%\n",
            "  Current best Val Acc: 0.8878205418586731\n",
            " --Epoch 5\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 5 LR: [0.03726186376279721]\n",
            "  \tTrain Loss: 0.592 | Train Acc: 79.54%\n",
            "  \t Val. Loss: 0.359 |  Val Acc: 88.86%\n",
            "  Current best Val Acc: 0.8886218070983887\n",
            " --Epoch 6\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 6 LR: [0.05791368658177225]\n",
            "  \tTrain Loss: 0.588 | Train Acc: 79.64%\n",
            "  \t Val. Loss: 0.334 |  Val Acc: 89.31%\n",
            "  Current best Val Acc: 0.8931289911270142\n",
            " --Epoch 7\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 7 LR: [0.08567627457823428]\n",
            "  \tTrain Loss: 0.583 | Train Acc: 79.67%\n",
            "  \t Val. Loss: 0.351 |  Val Acc: 88.63%\n",
            "  Current best Val Acc: 0.8931289911270142\n",
            " --Epoch 8\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 8 LR: [0.10305368692721564]\n",
            "  \tTrain Loss: 0.580 | Train Acc: 79.80%\n",
            "  \t Val. Loss: 0.366 |  Val Acc: 88.48%\n",
            "  Current best Val Acc: 0.8931289911270142\n",
            " --Epoch 9\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 9 LR: [0.1025085630942167]\n",
            "  \tTrain Loss: 0.577 | Train Acc: 79.96%\n",
            "  \t Val. Loss: 0.541 |  Val Acc: 83.78%\n",
            "  Current best Val Acc: 0.8931289911270142\n",
            " --Epoch 10\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 10 LR: [0.08386590697427362]\n",
            "  \tTrain Loss: 0.570 | Train Acc: 80.18%\n",
            "  \t Val. Loss: 0.392 |  Val Acc: 87.42%\n",
            "  Current best Val Acc: 0.8931289911270142\n",
            " --Epoch 11\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 11 LR: [0.053959611008004574]\n",
            "  \tTrain Loss: 0.569 | Train Acc: 80.24%\n",
            "  \t Val. Loss: 0.472 |  Val Acc: 85.69%\n",
            "  Current best Val Acc: 0.8931289911270142\n",
            " --Epoch 12\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 12 LR: [0.023872875703154883]\n",
            "  \tTrain Loss: 0.568 | Train Acc: 80.42%\n",
            "  \t Val. Loss: 0.419 |  Val Acc: 86.94%\n",
            "  Current best Val Acc: 0.8931289911270142\n",
            " --Epoch 13\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 13 LR: [0.004424211972106095]\n",
            "  \tTrain Loss: 0.563 | Train Acc: 80.48%\n",
            "  \t Val. Loss: 0.330 |  Val Acc: 89.49%\n",
            "  Current best Val Acc: 0.8949319124221802\n",
            " --Epoch 14\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 14 LR: [0.0]\n",
            "  \tTrain Loss: 0.560 | Train Acc: 80.59%\n",
            "  \t Val. Loss: 0.335 |  Val Acc: 89.24%\n",
            "  Current best Val Acc: 0.8949319124221802\n",
            " --Epoch 15\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 15 LR: [0.037261863762745664]\n",
            "  \tTrain Loss: 0.555 | Train Acc: 80.74%\n",
            "  \t Val. Loss: 0.318 |  Val Acc: 89.91%\n",
            "  Current best Val Acc: 0.8991386294364929\n",
            " --Epoch 16\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 16 LR: [0.057913686581662245]\n",
            "  \tTrain Loss: 0.554 | Train Acc: 80.86%\n",
            "  \t Val. Loss: 0.323 |  Val Acc: 89.66%\n",
            "  Current best Val Acc: 0.8991386294364929\n",
            " --Epoch 17\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 17 LR: [0.08567627457861902]\n",
            "  \tTrain Loss: 0.551 | Train Acc: 80.94%\n",
            "  \t Val. Loss: 0.309 |  Val Acc: 90.01%\n",
            "  Current best Val Acc: 0.9001402258872986\n",
            " --Epoch 18\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 18 LR: [0.1030536869270665]\n",
            "  \tTrain Loss: 0.549 | Train Acc: 80.91%\n",
            "  \t Val. Loss: 0.327 |  Val Acc: 89.56%\n",
            "  Current best Val Acc: 0.9001402258872986\n",
            " --Epoch 19\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 19 LR: [0.10250856309351593]\n",
            "  \tTrain Loss: 0.547 | Train Acc: 81.08%\n",
            "  \t Val. Loss: 0.363 |  Val Acc: 88.58%\n",
            "  Current best Val Acc: 0.9001402258872986\n",
            "--end running\n"
          ]
        }
      ],
      "source": [
        "best_result_SGD = (0.1, 128, 0.0005)\n",
        "lr, batch_size, wd = best_result_SGD\n",
        "\n",
        "N_EPOCHS = 20\n",
        "\n",
        "print(\"--------------lr={}, batch_size={}, wd={} start-------------\".format(lr, batch_size, wd))\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=wd)\n",
        "cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "transform1 = transforms.Compose([\n",
        "        \n",
        "        transforms.RandomResizedCrop(32),#训练模型有resize 和 翻折的操作\n",
        "    transforms.RandomHorizontalFlip(),#\n",
        "        transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "transform2 = transforms.Compose([\n",
        "        \n",
        "        transforms.RandomResizedCrop(32),#训练模型有resize 和 翻折的操作\n",
        "    transforms.RandomVerticalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "transform3 = transforms.Compose([\n",
        "        \n",
        "        transforms.RandomResizedCrop(32),#训练模型有resize 和 翻折的操作\n",
        "    transforms.RandomRotation(90),\n",
        "        transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainset1 = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform1)\n",
        "trainset2 = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform2)\n",
        "trainset3 = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform3)\n",
        "\n",
        "trainset = ConcatDataset([trainset,trainset1,trainset3,trainset2])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0,drop_last=True)\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0,drop_last=True)\n",
        "\n",
        "result = run_epoches(N_EPOCHS, model, optimizer, criterion, trainloader, testloader, cosine_scheduler)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "gem3ckGPx7uy",
        "outputId": "60e9d194-0a54-4490-8704-2125ef6bcdfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------lr=0.1, wd=0.0005, batch_size=128, start-------------\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "start running\n",
            " --Epoch 0\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 0 LR: [0.004894348370484647]\n",
            "  \tTrain Loss: 1.601 | Train Acc: 41.89%\n",
            "  \t Val. Loss: 1.218 |  Val Acc: 55.99%\n",
            "  Current best Val Acc: 0.559928834438324\n",
            " --Epoch 1\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 1 LR: [0.08386590697411969]\n",
            "  \tTrain Loss: 1.139 | Train Acc: 59.20%\n",
            "  \t Val. Loss: 2.041 |  Val Acc: 36.82%\n",
            "  Current best Val Acc: 0.559928834438324\n",
            " --Epoch 2\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 2 LR: [0.04448589487622449]\n",
            "  \tTrain Loss: 0.874 | Train Acc: 69.12%\n",
            "  \t Val. Loss: 0.813 |  Val Acc: 71.81%\n",
            "  Current best Val Acc: 0.7180577516555786\n",
            " --Epoch 3\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 3 LR: [0.05395961100810772]\n",
            "  \tTrain Loss: 0.700 | Train Acc: 75.52%\n",
            "  \t Val. Loss: 0.957 |  Val Acc: 67.35%\n",
            "  Current best Val Acc: 0.7180577516555786\n",
            " --Epoch 4\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 4 LR: [0.07236067977502762]\n",
            "  \tTrain Loss: 0.579 | Train Acc: 79.79%\n",
            "  \t Val. Loss: 0.700 |  Val Acc: 76.17%\n",
            "  Current best Val Acc: 0.7616693377494812\n",
            " --Epoch 5\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 5 LR: [0.02387287570313194]\n",
            "  \tTrain Loss: 0.489 | Train Acc: 83.02%\n",
            "  \t Val. Loss: 0.711 |  Val Acc: 75.90%\n",
            "  Current best Val Acc: 0.7616693377494812\n",
            " --Epoch 6\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 6 LR: [0.09629599990798779]\n",
            "  \tTrain Loss: 0.408 | Train Acc: 85.91%\n",
            "  \t Val. Loss: 0.914 |  Val Acc: 70.10%\n",
            "  Current best Val Acc: 0.7616693377494812\n",
            " --Epoch 7\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 7 LR: [0.0044242119720911514]\n",
            "  \tTrain Loss: 0.345 | Train Acc: 88.10%\n",
            "  \t Val. Loss: 0.727 |  Val Acc: 76.77%\n",
            "  Current best Val Acc: 0.7677017450332642\n",
            " --Epoch 8\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 8 LR: [0.10521243143695261]\n",
            "  \tTrain Loss: 0.283 | Train Acc: 90.29%\n",
            "  \t Val. Loss: 1.193 |  Val Acc: 65.73%\n",
            "  Current best Val Acc: 0.7677017450332642\n",
            " --Epoch 9\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 9 LR: [0.0]\n",
            "  \tTrain Loss: 0.241 | Train Acc: 91.74%\n",
            "  \t Val. Loss: 0.700 |  Val Acc: 78.59%\n",
            "  Current best Val Acc: 0.785897970199585\n",
            " --Epoch 10\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 10 LR: [0.09516553824443334]\n",
            "  \tTrain Loss: 0.196 | Train Acc: 93.36%\n",
            "  \t Val. Loss: 1.617 |  Val Acc: 63.44%\n",
            "  Current best Val Acc: 0.785897970199585\n",
            " --Epoch 11\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 11 LR: [0.03726186376262511]\n",
            "  \tTrain Loss: 0.163 | Train Acc: 94.51%\n",
            "  \t Val. Loss: 0.664 |  Val Acc: 80.75%\n",
            "  Current best Val Acc: 0.8074564933776855\n",
            " --Epoch 12\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 12 LR: [0.0696804401296078]\n",
            "  \tTrain Loss: 0.123 | Train Acc: 95.86%\n",
            "  \t Val. Loss: 1.527 |  Val Acc: 66.81%\n",
            "  Current best Val Acc: 0.8074564933776855\n",
            " --Epoch 13\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 13 LR: [0.057913686581790816]\n",
            "  \tTrain Loss: 0.107 | Train Acc: 96.52%\n",
            "  \t Val. Loss: 0.753 |  Val Acc: 79.46%\n",
            "  Current best Val Acc: 0.8074564933776855\n",
            " --Epoch 14\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 14 LR: [0.03819660112502122]\n",
            "  \tTrain Loss: 0.083 | Train Acc: 97.26%\n",
            "  \t Val. Loss: 0.852 |  Val Acc: 77.99%\n",
            "  Current best Val Acc: 0.8074564933776855\n",
            " --Epoch 15\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 15 LR: [0.08567627457815871]\n",
            "  \tTrain Loss: 0.077 | Train Acc: 97.40%\n",
            "  \t Val. Loss: 0.753 |  Val Acc: 80.66%\n",
            "  Current best Val Acc: 0.8074564933776855\n",
            " --Epoch 16\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 16 LR: [0.012295598939796277]\n",
            "  \tTrain Loss: 0.050 | Train Acc: 98.46%\n",
            "  \t Val. Loss: 0.774 |  Val Acc: 79.93%\n",
            "  Current best Val Acc: 0.8074564933776855\n",
            " --Epoch 17\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 17 LR: [0.10305368692687884]\n",
            "  \tTrain Loss: 0.040 | Train Acc: 98.78%\n",
            "  \t Val. Loss: 0.913 |  Val Acc: 77.97%\n",
            "  Current best Val Acc: 0.8074564933776855\n",
            " --Epoch 18\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 18 LR: [0.0006271407734226758]\n",
            "  \tTrain Loss: 0.036 | Train Acc: 98.93%\n",
            "  \t Val. Loss: 0.744 |  Val Acc: 82.11%\n",
            "  Current best Val Acc: 0.8211036324501038\n",
            " --Epoch 19\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 19 LR: [0.10250856309378148]\n",
            "  \tTrain Loss: 0.020 | Train Acc: 99.46%\n",
            "  \t Val. Loss: 0.760 |  Val Acc: 82.00%\n",
            "  Current best Val Acc: 0.8211036324501038\n",
            "--end running\n",
            "--------------lr=0.1, wd=0.0005, batch_size=128, result=0.8211036324501038-------------\n",
            "current best hyperparameters:(0.1, 128, 0.0005, tensor(0.8211, device='cuda:0'))\n",
            "--------------lr=0.1, wd=0.0005, batch_size=256, start-------------\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "start running\n",
            " --Epoch 0\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 0 LR: [0.08567627457812256]\n",
            "  \tTrain Loss: 1.675 | Train Acc: 39.53%\n",
            "  \t Val. Loss: 1.605 |  Val Acc: 41.93%\n",
            "  Current best Val Acc: 0.4193359315395355\n",
            " --Epoch 1\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 1 LR: [0.03726186376263812]\n",
            "  \tTrain Loss: 1.245 | Train Acc: 55.31%\n",
            "  \t Val. Loss: 1.044 |  Val Acc: 62.03%\n",
            "  Current best Val Acc: 0.620312511920929\n",
            " --Epoch 2\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 2 LR: [0.0044242119720878164]\n",
            "  \tTrain Loss: 0.988 | Train Acc: 64.80%\n",
            "  \t Val. Loss: 0.912 |  Val Acc: 67.49%\n",
            "  Current best Val Acc: 0.6749023795127869\n",
            " --Epoch 3\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 3 LR: [0.053959611008112265]\n",
            "  \tTrain Loss: 0.798 | Train Acc: 72.01%\n",
            "  \t Val. Loss: 2.064 |  Val Acc: 37.77%\n",
            "  Current best Val Acc: 0.6749023795127869\n",
            " --Epoch 4\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 4 LR: [0.10250856309367808]\n",
            "  \tTrain Loss: 0.661 | Train Acc: 76.85%\n",
            "  \t Val. Loss: 2.113 |  Val Acc: 44.88%\n",
            "  Current best Val Acc: 0.6749023795127869\n",
            " --Epoch 5\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 5 LR: [0.0856762745781386]\n",
            "  \tTrain Loss: 0.556 | Train Acc: 80.77%\n",
            "  \t Val. Loss: 0.774 |  Val Acc: 73.25%\n",
            "  Current best Val Acc: 0.7325195670127869\n",
            " --Epoch 6\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 6 LR: [0.03726186376265223]\n",
            "  \tTrain Loss: 0.459 | Train Acc: 84.04%\n",
            "  \t Val. Loss: 0.650 |  Val Acc: 77.52%\n",
            "  Current best Val Acc: 0.775195300579071\n",
            " --Epoch 7\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 7 LR: [0.004424211972089576]\n",
            "  \tTrain Loss: 0.379 | Train Acc: 86.93%\n",
            "  \t Val. Loss: 0.823 |  Val Acc: 73.54%\n",
            "  Current best Val Acc: 0.775195300579071\n",
            " --Epoch 8\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 8 LR: [0.053959611008103134]\n",
            "  \tTrain Loss: 0.305 | Train Acc: 89.61%\n",
            "  \t Val. Loss: 2.869 |  Val Acc: 38.38%\n",
            "  Current best Val Acc: 0.775195300579071\n",
            " --Epoch 9\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 9 LR: [0.10250856309373672]\n",
            "  \tTrain Loss: 0.263 | Train Acc: 91.05%\n",
            "  \t Val. Loss: 2.564 |  Val Acc: 48.35%\n",
            "  Current best Val Acc: 0.775195300579071\n",
            " --Epoch 10\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 10 LR: [0.08567627457808892]\n",
            "  \tTrain Loss: 0.206 | Train Acc: 93.11%\n",
            "  \t Val. Loss: 0.832 |  Val Acc: 74.89%\n",
            "  Current best Val Acc: 0.775195300579071\n",
            " --Epoch 11\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 11 LR: [0.037261863762673327]\n",
            "  \tTrain Loss: 0.163 | Train Acc: 94.66%\n",
            "  \t Val. Loss: 0.752 |  Val Acc: 78.04%\n",
            "  Current best Val Acc: 0.7803711295127869\n",
            " --Epoch 12\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 12 LR: [0.004424211972091609]\n",
            "  \tTrain Loss: 0.107 | Train Acc: 96.57%\n",
            "  \t Val. Loss: 0.935 |  Val Acc: 75.90%\n",
            "  Current best Val Acc: 0.7803711295127869\n",
            " --Epoch 13\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 13 LR: [0.053959611008107304]\n",
            "  \tTrain Loss: 0.084 | Train Acc: 97.40%\n",
            "  \t Val. Loss: 1.642 |  Val Acc: 66.33%\n",
            "  Current best Val Acc: 0.7803711295127869\n",
            " --Epoch 14\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 14 LR: [0.1025085630936854]\n",
            "  \tTrain Loss: 0.065 | Train Acc: 98.05%\n",
            "  \t Val. Loss: 1.519 |  Val Acc: 67.30%\n",
            "  Current best Val Acc: 0.7803711295127869\n",
            " --Epoch 15\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 15 LR: [0.0856762745781432]\n",
            "  \tTrain Loss: 0.078 | Train Acc: 97.50%\n",
            "  \t Val. Loss: 0.905 |  Val Acc: 77.00%\n",
            "  Current best Val Acc: 0.7803711295127869\n",
            " --Epoch 16\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 16 LR: [0.03726186376265534]\n",
            "  \tTrain Loss: 0.036 | Train Acc: 99.02%\n",
            "  \t Val. Loss: 0.831 |  Val Acc: 79.13%\n",
            "  Current best Val Acc: 0.791308581829071\n",
            " --Epoch 17\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 17 LR: [0.004424211972090067]\n",
            "  \tTrain Loss: 0.020 | Train Acc: 99.53%\n",
            "  \t Val. Loss: 0.897 |  Val Acc: 78.50%\n",
            "  Current best Val Acc: 0.791308581829071\n",
            " --Epoch 18\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 18 LR: [0.053959611008123354]\n",
            "  \tTrain Loss: 0.009 | Train Acc: 99.87%\n",
            "  \t Val. Loss: 0.828 |  Val Acc: 79.25%\n",
            "  Current best Val Acc: 0.79248046875\n",
            " --Epoch 19\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 19 LR: [0.1025085630937076]\n",
            "  \tTrain Loss: 0.003 | Train Acc: 99.99%\n",
            "  \t Val. Loss: 0.773 |  Val Acc: 80.77%\n",
            "  Current best Val Acc: 0.8077148795127869\n",
            "--end running\n",
            "--------------lr=0.1, wd=0.0005, batch_size=256, result=0.8077148795127869-------------\n",
            "current best hyperparameters:(0.1, 128, 0.0005, tensor(0.8211, device='cuda:0'))\n",
            "--------------lr=0.1, wd=0.0001, batch_size=128, start-------------\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "start running\n",
            " --Epoch 0\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 0 LR: [0.004894348370484647]\n",
            "  \tTrain Loss: 1.558 | Train Acc: 43.30%\n",
            "  \t Val. Loss: 1.158 |  Val Acc: 58.04%\n",
            "  Current best Val Acc: 0.5803995132446289\n",
            " --Epoch 1\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 1 LR: [0.08386590697411969]\n",
            "  \tTrain Loss: 1.035 | Train Acc: 63.16%\n",
            "  \t Val. Loss: 1.407 |  Val Acc: 53.08%\n",
            "  Current best Val Acc: 0.5803995132446289\n",
            " --Epoch 2\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 2 LR: [0.04448589487622449]\n",
            "  \tTrain Loss: 0.778 | Train Acc: 72.61%\n",
            "  \t Val. Loss: 0.734 |  Val Acc: 74.54%\n",
            "  Current best Val Acc: 0.7453520894050598\n",
            " --Epoch 3\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 3 LR: [0.05395961100810772]\n",
            "  \tTrain Loss: 0.633 | Train Acc: 77.94%\n",
            "  \t Val. Loss: 1.093 |  Val Acc: 65.21%\n",
            "  Current best Val Acc: 0.7453520894050598\n",
            " --Epoch 4\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 4 LR: [0.07236067977502762]\n",
            "  \tTrain Loss: 0.526 | Train Acc: 81.84%\n",
            "  \t Val. Loss: 0.651 |  Val Acc: 77.68%\n",
            "  Current best Val Acc: 0.7767998576164246\n",
            " --Epoch 5\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 5 LR: [0.02387287570313194]\n",
            "  \tTrain Loss: 0.437 | Train Acc: 84.77%\n",
            "  \t Val. Loss: 0.886 |  Val Acc: 72.31%\n",
            "  Current best Val Acc: 0.7767998576164246\n",
            " --Epoch 6\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 6 LR: [0.09629599990798779]\n",
            "  \tTrain Loss: 0.369 | Train Acc: 87.38%\n",
            "  \t Val. Loss: 0.735 |  Val Acc: 76.02%\n",
            "  Current best Val Acc: 0.7767998576164246\n",
            " --Epoch 7\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 7 LR: [0.0044242119720911514]\n",
            "  \tTrain Loss: 0.303 | Train Acc: 89.54%\n",
            "  \t Val. Loss: 0.767 |  Val Acc: 77.17%\n",
            "  Current best Val Acc: 0.7767998576164246\n",
            " --Epoch 8\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 8 LR: [0.10521243143695261]\n",
            "  \tTrain Loss: 0.243 | Train Acc: 91.57%\n",
            "  \t Val. Loss: 0.877 |  Val Acc: 74.76%\n",
            "  Current best Val Acc: 0.7767998576164246\n",
            " --Epoch 9\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 9 LR: [0.0]\n",
            "  \tTrain Loss: 0.200 | Train Acc: 93.08%\n",
            "  \t Val. Loss: 0.693 |  Val Acc: 79.45%\n",
            "  Current best Val Acc: 0.7945016026496887\n",
            " --Epoch 10\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 10 LR: [0.09516553824443334]\n",
            "  \tTrain Loss: 0.156 | Train Acc: 94.62%\n",
            "  \t Val. Loss: 1.769 |  Val Acc: 63.82%\n",
            "  Current best Val Acc: 0.7945016026496887\n",
            " --Epoch 11\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 11 LR: [0.03726186376262511]\n",
            "  \tTrain Loss: 0.147 | Train Acc: 94.92%\n",
            "  \t Val. Loss: 0.697 |  Val Acc: 79.46%\n",
            "  Current best Val Acc: 0.7946004867553711\n",
            " --Epoch 12\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 12 LR: [0.0696804401296078]\n",
            "  \tTrain Loss: 0.091 | Train Acc: 96.93%\n",
            "  \t Val. Loss: 0.956 |  Val Acc: 76.07%\n",
            "  Current best Val Acc: 0.7946004867553711\n",
            " --Epoch 13\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 13 LR: [0.057913686581790816]\n",
            "  \tTrain Loss: 0.069 | Train Acc: 97.76%\n",
            "  \t Val. Loss: 0.781 |  Val Acc: 80.01%\n",
            "  Current best Val Acc: 0.8001384735107422\n",
            " --Epoch 14\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 14 LR: [0.03819660112502122]\n",
            "  \tTrain Loss: 0.056 | Train Acc: 98.22%\n",
            "  \t Val. Loss: 1.129 |  Val Acc: 74.71%\n",
            "  Current best Val Acc: 0.8001384735107422\n",
            " --Epoch 15\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 15 LR: [0.08567627457815871]\n",
            "  \tTrain Loss: 0.052 | Train Acc: 98.30%\n",
            "  \t Val. Loss: 0.848 |  Val Acc: 79.67%\n",
            "  Current best Val Acc: 0.8001384735107422\n",
            " --Epoch 16\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 16 LR: [0.012295598939796277]\n",
            "  \tTrain Loss: 0.049 | Train Acc: 98.37%\n",
            "  \t Val. Loss: 0.884 |  Val Acc: 79.59%\n",
            "  Current best Val Acc: 0.8001384735107422\n",
            " --Epoch 17\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 17 LR: [0.10305368692687884]\n",
            "  \tTrain Loss: 0.044 | Train Acc: 98.60%\n",
            "  \t Val. Loss: 0.875 |  Val Acc: 79.78%\n",
            "  Current best Val Acc: 0.8001384735107422\n",
            " --Epoch 18\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 18 LR: [0.0006271407734226758]\n",
            "  \tTrain Loss: 0.048 | Train Acc: 98.39%\n",
            "  \t Val. Loss: 0.848 |  Val Acc: 80.43%\n",
            "  Current best Val Acc: 0.8042919635772705\n",
            " --Epoch 19\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 19 LR: [0.10250856309378148]\n",
            "  \tTrain Loss: 0.031 | Train Acc: 98.98%\n",
            "  \t Val. Loss: 0.888 |  Val Acc: 80.87%\n",
            "  Current best Val Acc: 0.808742105960846\n",
            "--end running\n",
            "--------------lr=0.1, wd=0.0001, batch_size=128, result=0.808742105960846-------------\n",
            "current best hyperparameters:(0.1, 128, 0.0005, tensor(0.8211, device='cuda:0'))\n",
            "--------------lr=0.1, wd=0.0001, batch_size=256, start-------------\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "start running\n",
            " --Epoch 0\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 0 LR: [0.08567627457812256]\n",
            "  \tTrain Loss: 1.681 | Train Acc: 38.65%\n",
            "  \t Val. Loss: 1.364 |  Val Acc: 49.05%\n",
            "  Current best Val Acc: 0.49052736163139343\n",
            " --Epoch 1\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 1 LR: [0.03726186376263812]\n",
            "  \tTrain Loss: 1.269 | Train Acc: 53.91%\n",
            "  \t Val. Loss: 1.106 |  Val Acc: 60.22%\n",
            "  Current best Val Acc: 0.602246105670929\n",
            " --Epoch 2\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 2 LR: [0.0044242119720878164]\n",
            "  \tTrain Loss: 1.036 | Train Acc: 62.70%\n",
            "  \t Val. Loss: 1.070 |  Val Acc: 61.88%\n",
            "  Current best Val Acc: 0.618847668170929\n",
            " --Epoch 3\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 3 LR: [0.053959611008112265]\n",
            "  \tTrain Loss: 0.826 | Train Acc: 70.92%\n",
            "  \t Val. Loss: 1.567 |  Val Acc: 49.02%\n",
            "  Current best Val Acc: 0.618847668170929\n",
            " --Epoch 4\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 4 LR: [0.10250856309367808]\n",
            "  \tTrain Loss: 0.686 | Train Acc: 76.23%\n",
            "  \t Val. Loss: 1.605 |  Val Acc: 51.71%\n",
            "  Current best Val Acc: 0.618847668170929\n",
            " --Epoch 5\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 5 LR: [0.0856762745781386]\n",
            "  \tTrain Loss: 0.577 | Train Acc: 79.81%\n",
            "  \t Val. Loss: 0.744 |  Val Acc: 74.23%\n",
            "  Current best Val Acc: 0.7422851920127869\n",
            " --Epoch 6\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 6 LR: [0.03726186376265223]\n",
            "  \tTrain Loss: 0.485 | Train Acc: 83.32%\n",
            "  \t Val. Loss: 0.639 |  Val Acc: 78.26%\n",
            "  Current best Val Acc: 0.7826172113418579\n",
            " --Epoch 7\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 7 LR: [0.004424211972089576]\n",
            "  \tTrain Loss: 0.402 | Train Acc: 86.23%\n",
            "  \t Val. Loss: 0.708 |  Val Acc: 76.28%\n",
            "  Current best Val Acc: 0.7826172113418579\n",
            " --Epoch 8\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 8 LR: [0.053959611008103134]\n",
            "  \tTrain Loss: 0.325 | Train Acc: 88.96%\n",
            "  \t Val. Loss: 1.402 |  Val Acc: 61.88%\n",
            "  Current best Val Acc: 0.7826172113418579\n",
            " --Epoch 9\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 9 LR: [0.10250856309373672]\n",
            "  \tTrain Loss: 0.267 | Train Acc: 91.02%\n",
            "  \t Val. Loss: 3.744 |  Val Acc: 34.00%\n",
            "  Current best Val Acc: 0.7826172113418579\n",
            " --Epoch 10\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 10 LR: [0.08567627457808892]\n",
            "  \tTrain Loss: 0.212 | Train Acc: 92.86%\n",
            "  \t Val. Loss: 0.884 |  Val Acc: 73.63%\n",
            "  Current best Val Acc: 0.7826172113418579\n",
            " --Epoch 11\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 11 LR: [0.037261863762673327]\n",
            "  \tTrain Loss: 0.162 | Train Acc: 94.68%\n",
            "  \t Val. Loss: 0.708 |  Val Acc: 79.52%\n",
            "  Current best Val Acc: 0.795214831829071\n",
            " --Epoch 12\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 12 LR: [0.004424211972091609]\n",
            "  \tTrain Loss: 0.114 | Train Acc: 96.36%\n",
            "  \t Val. Loss: 0.828 |  Val Acc: 77.17%\n",
            "  Current best Val Acc: 0.795214831829071\n",
            " --Epoch 13\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 13 LR: [0.053959611008107304]\n",
            "  \tTrain Loss: 0.084 | Train Acc: 97.36%\n",
            "  \t Val. Loss: 1.623 |  Val Acc: 65.32%\n",
            "  Current best Val Acc: 0.795214831829071\n",
            " --Epoch 14\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 14 LR: [0.1025085630936854]\n",
            "  \tTrain Loss: 0.065 | Train Acc: 98.03%\n",
            "  \t Val. Loss: 1.119 |  Val Acc: 74.09%\n",
            "  Current best Val Acc: 0.795214831829071\n",
            " --Epoch 15\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 15 LR: [0.0856762745781432]\n",
            "  \tTrain Loss: 0.042 | Train Acc: 98.83%\n",
            "  \t Val. Loss: 1.073 |  Val Acc: 74.91%\n",
            "  Current best Val Acc: 0.795214831829071\n",
            " --Epoch 16\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 16 LR: [0.03726186376265534]\n",
            "  \tTrain Loss: 0.016 | Train Acc: 99.73%\n",
            "  \t Val. Loss: 0.817 |  Val Acc: 80.34%\n",
            "  Current best Val Acc: 0.803417980670929\n",
            " --Epoch 17\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 17 LR: [0.004424211972090067]\n",
            "  \tTrain Loss: 0.007 | Train Acc: 99.93%\n",
            "  \t Val. Loss: 0.826 |  Val Acc: 80.50%\n",
            "  Current best Val Acc: 0.804980456829071\n",
            " --Epoch 18\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 18 LR: [0.053959611008123354]\n",
            "  \tTrain Loss: 0.003 | Train Acc: 100.00%\n",
            "  \t Val. Loss: 0.870 |  Val Acc: 79.39%\n",
            "  Current best Val Acc: 0.804980456829071\n",
            " --Epoch 19\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 19 LR: [0.1025085630937076]\n",
            "  \tTrain Loss: 0.002 | Train Acc: 99.99%\n",
            "  \t Val. Loss: 0.807 |  Val Acc: 81.35%\n",
            "  Current best Val Acc: 0.8134765625\n",
            "--end running\n",
            "--------------lr=0.1, wd=0.0001, batch_size=256, result=0.8134765625-------------\n",
            "current best hyperparameters:(0.1, 128, 0.0005, tensor(0.8211, device='cuda:0'))\n",
            "--------------lr=0.05, wd=0.0005, batch_size=128, start-------------\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "start running\n",
            " --Epoch 0\n",
            " --start training--\n",
            " --start validing--\n",
            "Epoch: 0 LR: [0.0024471741852423235]\n",
            "  \tTrain Loss: 1.484 | Train Acc: 45.48%\n",
            "  \t Val. Loss: 1.139 |  Val Acc: 58.56%\n",
            "  Current best Val Acc: 0.5856408476829529\n",
            " --Epoch 1\n",
            " --start training--\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 20\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "lr_candidates = [0.1, 0.05, 0.005]\n",
        "wd_candidates = [5e-4,1e-4]\n",
        "\n",
        "batch_size_candidates = [128, 256]\n",
        "\n",
        "best_result = (0,0,0,0)\n",
        "for lr in lr_candidates:\n",
        "  for wd in wd_candidates:\n",
        "    for batch_size in batch_size_candidates:\n",
        "      print(\"--------------lr={}, wd={}, batch_size={}, start-------------\".format(lr, wd, batch_size))\n",
        "      # optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "      model = ResNet(BuildingBlock, [3,3,2,2], 3, 32, 10)\n",
        "      model = model.to(device)\n",
        "      optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=wd)\n",
        "      cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
        "\n",
        "      trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "      trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "      testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "      testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "      result = run_epoches(N_EPOCHS, model, optimizer,  criterion, trainloader, testloader， cosine_scheduler)\n",
        "      print(\"--------------lr={}, wd={}, batch_size={}, result={}-------------\".format(lr, wd, batch_size, result))\n",
        "      if result > best_result[3]:\n",
        "        best_result = (lr, batch_size, wd, result)\n",
        "      print('current best hyperparameters:{}'.format(best_result))\n",
        "    \n",
        "print(best_result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSsHRL/+bAEIPrrnfyb5GL",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}